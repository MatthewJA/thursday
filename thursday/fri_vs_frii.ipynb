{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example FRI vs FRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "import keras \n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "\n",
    "from format_data import add_random\n",
    "from format_data import append_random\n",
    "from format_data import augment\n",
    "from format_data import generate_labels_fri\n",
    "from format_data import generate_labels_frii\n",
    "from format_data import get_aniyan\n",
    "from format_data import get_fr\n",
    "from format_data import get_random\n",
    "from format_data import initial_resizing\n",
    "from format_data import resize_array\n",
    "from models import HOGNet\n",
    "from models import SklearnModel\n",
    "from query import download_fr_components\n",
    "from query import download_random\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Seed\n",
    "seed = 0\n",
    "\n",
    "data_path = 'data/'\n",
    "save_path = 'saved/'\n",
    "\n",
    "# Setting Paths\n",
    "fr_data_path = data_path + 'data.h5'\n",
    "random_path = data_path + 'random1000.h5'\n",
    "everything_path = data_path + 'everything.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting Raw Data\n",
    "\n",
    "Here, if the files do not already exist, we convert the output of fri-frii-download.ipynb as well as a h5py file of random sources into the required format, and dump them new h5py files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(fr_data_path).is_file():\n",
    "    download_fr_components(fr_data_path)\n",
    "\n",
    "if not Path(random_path).is_file():\n",
    "    download_random(random_path, n=1000)\n",
    "\n",
    "if not Path(everything_path).is_file():\n",
    "    add_random(fr_data_path, random_path, everything_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting indices of training and testing data \n",
    "\n",
    "We use the get_fr function to get locations of the training and testing data from our FR data file.\n",
    "\n",
    "Classes are split by split_ratio, with split_ratio x class going to the test set and (1 - split_ratio) x class going to train set. The training and testing indices are outputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_i, test_i = get_fr(everything_path, split_ratio=(1/3), seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the data generator \n",
    "Using the augment_data construxt our keras.preprocessing.image.ImageDataGenerator object to apply randomly augmentations to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = augment(rotation_range=180, zoom_range=0.2, shift_range=0.0, flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating Sklearn Classifier\n",
    "We use the SklearnModel class to construct our random forest classifer. Any other Sklearn Classifier can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rft = SklearnModel(RandomForestClassifier, datagen=datagen, nb_augment=100, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifer\n",
    "We train our classifer with the fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(everything_path, 'r') as data:\n",
    "    train_x = np.asarray(data['images'])[train_i]\n",
    "    test_x = np.asarray(data['images'])[test_i]\n",
    "    labels = np.asarray(data['labels'])\n",
    "    images = np.asarray(data['images'])\n",
    "    \n",
    "    # Formatting images\n",
    "    train_x = resize_array(train_x, dim=256)\n",
    "    train_x = np.expand_dims(train_x, axis=3)\n",
    "    train_x = normalize_pixels(train_x)\n",
    "    test_x = resize_array(test_x, dim=256)\n",
    "    test_x = np.expand_dims(test_x, axis=3)\n",
    "    test_x = normalize_pixels(test_x)\n",
    "    \n",
    "    # Formatting labels\n",
    "    train_y = np.where(labels[train_i]==1, False, True)\n",
    "    test_y = np.where(labels[test_i]==1, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(train_x.shape[0]):\n",
    "    plt.imshow(train_x[i, :, :, 0]) #cmap=\"viridis\"\n",
    "    plt.show()\n",
    "    print (train_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rft.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rft.save(path=save_path + 'rft.pk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Classifer to Pickle File\n",
    "Using the save method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rft.load(path=save_path + \"rft.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rft.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rft.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rft.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Unseen Samples\n",
    "First, we load our model with the load method (If the save method has been used on the same script, path is inferred). We test our classifier with the predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rft = rft.load()\n",
    "\n",
    "predictions = rft.predict_proba(test_x)\n",
    "test_y = to_categorical(test_y, 2)\n",
    "\n",
    "correct = test_y == predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating HOGNet Model\n",
    "We use the HOGNet class to construct our custom keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hognet = HOGNet(datagen=datagen, batch_size=50, steps_per_epoch=1, max_epoch=1, patience=5, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifer\n",
    "We train our classifer with the fit method. Model stops training when loss stop decreasing for a set number of epochs (patience argument controls this) or when it reaches the maximum number of epochs. The amount of data augmented by datagen is dictated by the batch_size, and how many batches are generated per epoch is controlled by steps_per_epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hognet.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hognet.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hognet.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hognet.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model Weights to H5py file\n",
    "Using the save method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hognet.save(path=\"data/hognet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Unseen Samples\n",
    "First, we load our weights into the model with the load method (If the save method has been used on the same script, path is inferred). We test our classifier with the predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hognet.load()\n",
    "\n",
    "predictions = hognet.predict_proba(test_x)\n",
    "\n",
    "#correct = test_y == predictions\n",
    "#print('Accuracy: {:.02%}'.format(correct.mean()))\n",
    "print (predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
