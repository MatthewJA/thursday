{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage of Thursday API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fires\\Anaconda3\\envs\\CSIRO\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "import keras \n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from format_data import get_data, get_fr_data, augment_data, resize_array\n",
    "from models import SklearnModel, HOGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data output by fri_frii_download.ipynb.\n",
    "fr_data_path = \"data/data.h5\"\n",
    "# Path to Aniyan+17 validation set.\n",
    "aniyan_path = \"data/asu.tsv\"\n",
    "\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting indices of training and testing data \n",
    "\n",
    "We use the get_data function to get locations of the training and testing data from our FR data and Aniyan data\n",
    "\n",
    "Coordinates from data.h5 and asu.tsv are converted into Skycoords. The two are cross-matched. Sources within five arcsec radius of each other are assumed to be the same object. Aniyan sources are extracted from the FR sources, and formated into class-wise balanced training set. The remainder are formated into class-wise balanced testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_i, test_i = get_data(fr_data_path, aniyan_path, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the data generator \n",
    "Using the augment_data construxt our keras.preprocessing.image.ImageDataGenerator object to apply randomly augmentations to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = augment_data(rotation_range=180, zoom_range=0.2, shift_range=0.0, flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating Sklearn Classifier\n",
    "We use the SklearnModel class to construct our random forest classifer. Any other Sklearn Classifier can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rft = SklearnModel(LogisticRegression, datagen=datagen, nb_augment=10, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifer\n",
    "We train our classifer with the fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with h5py.File(fr_data_path, 'r') as data:\n",
    "    train_x = np.asarray(data['images'])[train_i]\n",
    "    train_y = np.asarray(data['labels'])[train_i]\n",
    "    \n",
    "    # Formatting images\n",
    "    train_x = resize_array(train_x, dim=256)\n",
    "    train_x = np.expand_dims(train_x, axis=3)\n",
    "    \n",
    "    rft.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Classifer to Pickle File\n",
    "Using the save method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifer\n",
    "We train our classifer with the fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rft.save(path=\"data/rft.pk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Unseen Samples\n",
    "First, we load our model with the load method (If the save method has been used on the same script, path is inferred). We test our classifier with the predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.SklearnModel at 0x157d00d86d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rft.save(path=\"data/rft.pk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Unseen Samples\n",
    "First, we load our model with the load method (If the save method has been used on the same script, path is inferred). We test our classifier with the predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "rtf = rft.load()\n",
    "\n",
    "with h5py.File(fr_data_path, 'r') as data:\n",
    "    test_x = np.asarray(data['images'])[test_i]\n",
    "    test_y = np.asarray(data['labels'])[test_i]\n",
    "    \n",
    "    # Formatting images\n",
    "    test_x = resize_array(test_x, dim=256)\n",
    "    test_x = np.expand_dims(test_x, axis=3)\n",
    "    \n",
    "    predictions = rtf.predict(test_x)\n",
    "    \n",
    "    correct = test_y == predictions\n",
    "    print('Accuracy: {:.02%}'.format(correct.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating HOGNet Model\n",
    "We use the HOGNet class to construct our custom keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hognet = HOGNet(datagen=datagen, batch_size=10, steps_per_epoch=50, max_epoch=1, patience=5, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifer\n",
    "We train our classifer with the fit method. Model stops training when loss stop decreasing for a set number of epochs (patience argument controls this) or when it reaches the maximum number of epochs. The amount of data augmented by datagen is dictated by the batch_size, and how many batches are generated per epoch is controlled by steps_per_epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fires\\Desktop\\CSIRO\\thursday\\thursday\\models.py:422: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, kernel_size=(1, 1), strides=(1, 1), activation=\"relu\", trainable=True, name=\"votes\", use_bias=False)`\n",
      "  trainable=True, bias=False, name=\"votes\")(sin_cos_vec)\n"
     ]
    }
   ],
   "source": [
    "hognet = HOGNet(datagen=datagen, batch_size=10, steps_per_epoch=50, max_epoch=1, patience=5, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hognet.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model Weights to H5py file\n",
    "Using the save method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50/50 [==============================] - 53s 1s/step - loss: 15.9424 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.HOGNet at 0x157d00d87b8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hognet.save(path=\"data/hognet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Unseen Samples\n",
    "First, we load our weights into the model with the load method (If the save method has been used on the same script, path is inferred). We test our classifier with the predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.HOGNet at 0x157d00d87b8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hognet.save(path=\"data/hognet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hognet.load()\n",
    "\n",
    "predictions = hognet.predict(test_x)\n",
    "\n",
    "correct = test_y == predictions\n",
    "print('Accuracy: {:.02%}'.format(correct.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
