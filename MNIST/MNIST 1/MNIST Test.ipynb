{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "import six.moves.cPickle as pickle\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The data shuffled and split to test and train sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Reshaping and normalising the inputs\n",
    "input_dim = 784  #28*28\n",
    "X_train = X_train.reshape(60000, input_dim)\n",
    "X_test = X_test.reshape(10000, input_dim)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "nb_classes = 10\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print(np.shape(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dim = nb_classes\n",
    "input_dim = 784\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 10, input_dim = input_dim, activation='softmax'))\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "batch_size = 128\n",
    "batch_size = 128\n",
    "np_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fires\\Anaconda3\\envs\\python3.5\\lib\\site-packages\\keras\\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 1s - loss: 2.2835 - acc: 0.1678 - val_loss: 2.2560 - val_acc: 0.2746\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 0s - loss: 2.2277 - acc: 0.3920 - val_loss: 2.1917 - val_acc: 0.45010\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 0s - loss: 2.1563 - acc: 0.4596 - val_loss: 2.1095 - val_acc: 0.4659\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 0s - loss: 2.0679 - acc: 0.4586 - val_loss: 2.0113 - val_acc: 0.4661\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 0s - loss: 1.9672 - acc: 0.4620 - val_loss: 1.9051 - val_acc: 0.4722\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s - loss: 1.8630 - acc: 0.4689 - val_loss: 1.8011 - val_acc: 0.4761\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 0s - loss: 1.7641 - acc: 0.4762 - val_loss: 1.7055 - val_acc: 0.4811\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 0s - loss: 1.6747 - acc: 0.4817 - val_loss: 1.6207 - val_acc: 0.4869\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 0s - loss: 1.5959 - acc: 0.4861 - val_loss: 1.5465 - val_acc: 0.4907\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 0s - loss: 1.5269 - acc: 0.4900 - val_loss: 1.4815 - val_acc: 0.4965\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 0s - loss: 1.4667 - acc: 0.4954 - val_loss: 1.4249 - val_acc: 0.5003\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 0s - loss: 1.4140 - acc: 0.4996 - val_loss: 1.3756 - val_acc: 0.5040\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s - loss: 1.3682 - acc: 0.5036 - val_loss: 1.3329 - val_acc: 0.5085\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s - loss: 1.3284 - acc: 0.5078 - val_loss: 1.2960 - val_acc: 0.5119\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s - loss: 1.2940 - acc: 0.5198 - val_loss: 1.2644 - val_acc: 0.5171\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s - loss: 1.2644 - acc: 0.5309 - val_loss: 1.2369 - val_acc: 0.5334\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s - loss: 1.2387 - acc: 0.5298 - val_loss: 1.2136 - val_acc: 0.5302\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s - loss: 1.2166 - acc: 0.5324 - val_loss: 1.1933 - val_acc: 0.5260\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s - loss: 1.1973 - acc: 0.5285 - val_loss: 1.1757 - val_acc: 0.5260\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s - loss: 1.1806 - acc: 0.5301 - val_loss: 1.1606 - val_acc: 0.5238\n",
      "Test score: 1.16056947002\n",
      "Test accuracy: 0.5238\n"
     ]
    }
   ],
   "source": [
    "sgd = optimizers.SGD(lr=0.01)\n",
    "\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, \n",
    "                    nb_epoch=np_epoch, verbose=1, \n",
    "                    validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()  #as json\n",
    "open('mnist_Logistic_model.json', 'w').write(json_string)\n",
    "\n",
    "yaml_string = model.to_yaml()  #as yaml\n",
    "open('mnist_Logistic_model.yaml', 'w').write(yaml_string)\n",
    "\n",
    "# save the weights in h5 format\n",
    "model.save_weights('mnist_Logistic_wts.h5')\n",
    "\n",
    "\n",
    "# uncomment the code below (and modify accordingly) to read a saved model and weights \n",
    "# model = model_from_json(open('my_model_architecture.json').read())# if json \n",
    "# model.load_weights('my_model_weights.h5')\n",
    "\n",
    "# model = model_from_yaml(open('my_model_architecture.yaml').read())# if yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
