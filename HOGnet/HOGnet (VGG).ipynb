{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduce a tensor-based implementation of the HOG feature extractor, built in Keras and Tensorflow. It is, to our knowledge, the first of its kind. To restrict all operations to the tensor domain, we approximate the previously employed histogram bin construction methodology, known as voting via bi-linear interpolation, as the scalar projection of each angle vector onto each separate bin unit vector. Since this is defined as a simple dot product between the two aforementioned vectors, we can implement this as a Convolution2D operation, with the angle vectors as the input and the bin unit vectors as the filters, with each filter representing a single unit vector that operates on every angle vector in the image. A Relu activation function operates on the output to curb the influence of negative projections. The reshaping required for the bin operations broken into separate depthwise convolution2D operations (via Tensorflow) followed via a Concatenation. \n",
    "\n",
    "As both Tensorflow allows the automatic computation of gradients, unlike the SkImage HOG, our Tensor-based HOG enables backpropagation through HOG. This opens up an array of previously unexplored possibilities. It is for instance now possible to have HOG as an intermediary layer, meaning that instead of having the raw images as inputs, we first pass them through a series of convolution layers and then into HOG, theoretically enabling the creation of a more powerful feature extractor. We shall test this premise in the not too distant future.\n",
    "\n",
    "For this notebook, we simply compare the performance of the outputs of the flattened bin array on both a keras dense logictic regiression and a Scipy logistic regression. We use centered samples. The results show that a simple scipy logictic regression on the output predictions prerforms significantly better than the intergrated keras Dense layer. Our current aim is to correct this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "#Importing packages\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, Input, Dense, Lambda, multiply, AveragePooling2D, Concatenate, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from keras.engine.topology import Layer\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import function\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "K.set_image_dim_ordering('tf')\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening pre-configured training and testing data.\n",
    "# Balanced classes and equal test/train split\n",
    "# Order: Samples are balanced, shuffled, split into training and testing sets, and then fed into the data generator separately.\n",
    "# Images are geneated through Keras' ImageDataGenerator, with the following settings:\n",
    "# rotation_range=180, zoom_range=0.2, horizontal_flip=True, vertical_flip =True, fill_mode = \"nearest\")\n",
    "\n",
    "file = open(\"train_y_VGG.dat\",'rb')\n",
    "train_y_n = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"test_y_VGG.dat\",'rb')\n",
    "test_y_n = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"train_x_VGG.dat\",'rb')\n",
    "train_x_n = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"test_x_VGG.dat\",'rb')\n",
    "test_x_n = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6150, 384, 384)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Values\n",
    "bins = 8         # number of bins in histogram\n",
    "w = 2*np.pi/bins     # width of each bin\n",
    "centers = np.arange(-np.pi, np.pi, w) + 0.5*w   #centers of each bin\n",
    "\n",
    "cell_size = 8   # Each cell will contain cell_size^2 pixels\n",
    "block_size = 2  # Each block will contain block_size^2 pixels\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train  = train_x_n.shape[0]\n",
    "nb_test  = test_x_n.shape[0]\n",
    "\n",
    "# Prosessing images to ensure there are no vanishing and exploding gradients\n",
    "train_x_n += (10e-10)*np.random.randn(nb_train, 384, 384)\n",
    "test_x_n += (10e-10)*np.random.randn(nb_test, 384, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring samples are divisible by batch size\n",
    "if train_x_n.shape[0]%batch_size != 0:\n",
    "    \n",
    "    # Slicing off end of samples to ensure train_y.shape[0]%batch_size == 0\n",
    "    train_y = train_y_n[:-(train_y_n.shape[0]%batch_size)]\n",
    "    test_y = test_y_n[:-(test_y_n.shape[0]%batch_size)]\n",
    "    \n",
    "    # Slicing off end of samples to ensure train_x.shape[0]%batch_size == 0\n",
    "    # Adding dimention for the colour chanels (It's greyscale, hence only 1)\n",
    "    train_x = train_x_n.reshape((train_x_n.shape[0], train_x_n.shape[1], train_x_n.shape[2], 1))[:-(train_x_n.shape[0]%batch_size), :, :, :]\n",
    "    test_x = test_x_n.reshape((test_x_n.shape[0], test_x_n.shape[1], test_x_n.shape[2], 1))[:-(test_x_n.shape[0]%batch_size), :, :, :]\n",
    "\n",
    "# Special case (-0 indices returns zero images)\n",
    "elif train_x_n.shape[0]%batch_size == 0:\n",
    "    train_y = train_y_n\n",
    "    test_y = test_y_n\n",
    "    \n",
    "    train_x = train_x_n.reshape((train_x_n.shape[0], train_x_n.shape[1], train_x_n.shape[2], 1))\n",
    "    test_x = test_x_n.reshape((test_x_n.shape[0], test_x_n.shape[1], test_x_n.shape[2], 1))\n",
    "    \n",
    "else:\n",
    "    print (\"Biggly Error\")\n",
    "\n",
    "# Convert class vectors to binary class matrices (for Keras Dense)\n",
    "conf_train_y = np_utils.to_categorical(train_y, 2)\n",
    "conf_test_y = np_utils.to_categorical(test_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6100, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining Gradient functions to fix vanishing and exploding gradients\n",
    "@function.Defun(tf.float32, tf.float32)\n",
    "def norm_grad(x, dy):\n",
    "    return dy*(x/tf.norm(x))\n",
    "\n",
    "@function.Defun(tf.float32, grad_func=norm_grad)\n",
    "def norm(x):\n",
    "    return tf.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Weights for the vertical and horizontal convolutions to calculate the image gradients\n",
    "prewitt_x = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]).reshape((1, 3, 3, 1, 1)) + 0.01*np.random.randn(1, 3, 3, 1, 1)\n",
    "prewitt_y = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]).reshape((1, 3, 3, 1, 1)) + 0.01*np.random.randn(1, 3, 3, 1, 1)\n",
    "\n",
    "cent = np.vstack((np.sin(centers), np.cos(centers))).reshape((1, 1, 1, 2, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[-0.99877557]],\n",
       "\n",
       "         [[-0.00129192]],\n",
       "\n",
       "         [[ 0.9788288 ]]],\n",
       "\n",
       "\n",
       "        [[[-1.00167166]],\n",
       "\n",
       "         [[-0.01058604]],\n",
       "\n",
       "         [[ 0.99762672]]],\n",
       "\n",
       "\n",
       "        [[[-0.99788489]],\n",
       "\n",
       "         [[-0.00801211]],\n",
       "\n",
       "         [[ 1.00165138]]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prewitt_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Filters for the Bin Operations\n",
    "def create_bin_filters(bin_dim):\n",
    "    filters = np.zeros((bin_dim**2, bin_dim, bin_dim))\n",
    "\n",
    "    count = 0 \n",
    "    for i in range(bin_dim):\n",
    "        for j in range(bin_dim):\n",
    "            filters[count, i, j] = 1\n",
    "            count += 1\n",
    "    return filters   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_dim must be 2\n",
    "# Increasing this will require the adding of some more tf.nn.depthwise_conv2d functions. There is one for each element in a single filter (i.e bin_dim^2)\n",
    "\n",
    "b_flt = create_bin_filters(2)\n",
    "bin_filters_n = b_flt.reshape((b_flt.shape[0], b_flt.shape[1], b_flt.shape[2], 1)).repeat(bins, 3) # Simply copying each filter anlong the last axis to satisfy the required shape for weights array (see Tensorflow docs for tf.nn.depthwise_conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping to satisfy required shape for weight array\n",
    "bin_filters = bin_filters_n.reshape(bin_filters_n.shape[0], bin_filters_n.shape[1], bin_filters_n.shape[2], bin_filters_n.shape[3], 1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_filters[0, :, :, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 3, 1, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prewitt_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_prewitt_x(shape, dtype=None):\n",
    "    shp = prewitt_x.shape\n",
    "    weight = K.variable(prewitt_x) + K.random_normal_variable(shp, mean=0.0, scale=0.001, dtype=dtype)\n",
    "    return weight\n",
    "\n",
    "def init_prewitt_y(shape, dtype=None):\n",
    "    shp = prewitt_y.shape\n",
    "    weight = K.variable(prewitt_y) + K.random_normal_variable(shp, mean=0.0, scale=0.001, dtype=dtype)\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model HyperParameters\n",
    "cell_dim = 8\n",
    "bin_dim = 2\n",
    "bin_stride_length = 1\n",
    "bs = bin_stride_length\n",
    "\n",
    "# Number of cells along each dim \n",
    "cell_nb = 128//cell_dim\n",
    "assert not 128 % cell_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt1 = tf.convert_to_tensor(bin_filters[0, :, :, :, :])\n",
    "filt2 = tf.convert_to_tensor(bin_filters[1, :, :, :, :])\n",
    "filt3 = tf.convert_to_tensor(bin_filters[2, :, :, :, :])\n",
    "filt4 = tf.convert_to_tensor(bin_filters[3, :, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef bins_flatten(bins):\\n    bins_flatten = tf.reshape(bins, (-1))\\n    return bins_flatten\\n\\ndef bin_flatten_output_shape(input_shape):\\n    shape = list(input_shape)\\n    assert len(shape) == 4  #only valid for 4D tensors\\n    shape = shape[0]*shape[1]*shape[2]*shape[3]\\n    return tuple(shape)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_magnitudes(conv_stacked):\n",
    "    #b_bound = 10.0e-10\n",
    "    #b_fill = tf.fill(tf.shape(conv_stacked), tf.constant(b_bound))\n",
    "    \n",
    "    #mag = tf.where(tf.abs(conv_stacked) < b_bound, b_fill, conv_stacked)\n",
    "    mags = tf.norm((conv_stacked), axis=3)\n",
    "    return mags\n",
    "\n",
    "def calculate_angles(conv_stacked):\n",
    "    angles = tf.atan2(conv_stacked[:, :, :, 1], conv_stacked[:, :, :, 0])\n",
    "    return angles\n",
    "\n",
    "def calculate_sin_cos(angles):\n",
    "    sin = K.sin(angles)\n",
    "    cos = K.cos(angles)\n",
    "    sin_cos = K.stack([sin, cos], axis =-1)\n",
    "    return sin_cos\n",
    "   \n",
    "def bins1(cells):\n",
    "    c_bins = tf.nn.depthwise_conv2d(cells, filt1, strides = (1, bs, bs, 1), padding=\"SAME\")\n",
    "    return c_bins\n",
    "\n",
    "def bins2(cells):\n",
    "    c_bins = tf.nn.depthwise_conv2d(cells, filt2, strides = (1, bs, bs, 1), padding=\"SAME\")\n",
    "    return c_bins \n",
    "\n",
    "def bins3(cells):\n",
    "    c_bins = tf.nn.depthwise_conv2d(cells, filt3, strides = (1, bs, bs, 1), padding=\"SAME\")\n",
    "    return c_bins\n",
    "\n",
    "def bins4(cells):\n",
    "    c_bins = tf.nn.depthwise_conv2d(cells, filt4, strides = (1, bs, bs, 1), padding=\"SAME\")\n",
    "    return c_bins\n",
    "\n",
    "def bin_norm(bins_layer):\n",
    "    bins_norm = tf.div(bins_layer, tf.expand_dims(tf.sqrt(tf.norm(bins_layer, axis=-1)+0.00000001), -1))\n",
    "    return bins_norm\n",
    "\n",
    "def hog_norm(bins_layer):\n",
    "    hog_norms = tf.div(bins_layer, tf.expand_dims(tf.sqrt(tf.norm(bins_layer, axis=-1)+0.00000001), -1))\n",
    "    hog_norms_2 = tf.div(hog_norms, tf.expand_dims(tf.sqrt(tf.norm(hog_norms, axis=-1)+0.00000001), -1))\n",
    "    return hog_norms_2\n",
    "\n",
    "def bins_flatten(bins_norm):\n",
    "    bins_flattened = tf.reshape(bins_norm, (batch_size, (cell_nb-bs)*(cell_nb-bs)*bins*(bin_dim**2)))\n",
    "    return bins_flattened\n",
    "\n",
    "def tensorflow_dense(bins_flattened):\n",
    "    logistic = tf.layers.dense(inputs=bins_flattened, units = 2, activation=\"sigmoid\")\n",
    "    return logistic\n",
    "\n",
    "\"\"\"\n",
    "def bins_flatten(bins):\n",
    "    bins_flatten = tf.reshape(bins, (-1))\n",
    "    return bins_flatten\n",
    "\n",
    "def bin_flatten_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 4  #only valid for 4D tensors\n",
    "    shape = shape[0]*shape[1]*shape[2]*shape[3]\n",
    "    return tuple(shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_shape = batch_size*(cell_nb-bs)*(cell_nb-bs)*bins*(bin_dim**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6100, 384, 384, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, kernel_size=(1, 1), strides=(1, 1), activation=\"relu\", trainable=False, name=\"votes\", use_bias=False)`\n"
     ]
    }
   ],
   "source": [
    "# Building Model \n",
    "inputs = Input(shape = (384, 384, 1), name=\"input\")\n",
    "\n",
    "# Convolutions\n",
    "x_conv = Conv2D(1, (3,3), strides=1, padding=\"same\", data_format=\"channels_last\", trainable=False, use_bias=False, name=\"conv_x\")(inputs)\n",
    "y_conv = Conv2D(1, (3,3), strides=1, padding=\"same\", data_format=\"channels_last\", trainable=False, use_bias=False, name=\"conv_y\")(inputs)\n",
    "\n",
    "# Stacking since it appears you cannot have more than a single input into a layer (i.e mags and angles)\n",
    "conv_stacked = Concatenate(axis=-1, name=\"Conv_Stacked\")([x_conv, y_conv])\n",
    "\n",
    "# Calculating the gradient magnitudes and angles\n",
    "mags = Lambda(calculate_magnitudes, output_shape=(384, 384), name=\"mags1\")(conv_stacked)\n",
    "mags = Lambda(lambda x: K.stack((mags, mags), axis=-1), name=\"mags2\")(mags)   # To enable sin_cos_vec \n",
    "angles = Lambda(calculate_angles, output_shape=(384, 384), name=\"angles\")(conv_stacked)\n",
    "\n",
    "# Calculating the components of angles in the x and y direction\n",
    "# Then multiplying by magnitudes, giving angle vectors\n",
    "sin_cos = Lambda(calculate_sin_cos, output_shape=(384, 384, 2), name=\"sin_cos\")(angles)\n",
    "sin_cos_vec = multiply([sin_cos, mags], name=\"sin_cos_mag\")\n",
    "\n",
    "# Applying each filter (representing a single bin unit vector) to every angle vector in the image.\n",
    "# Result is an array with shape (img_height, img_width, bins) where each bin is contains an angle vectors contribution to each bin\n",
    "# Relu activation function to remove negative projections (represented by negative scalar dot products).\n",
    "votes = Conv2D(8, kernel_size=(1, 1), strides=(1, 1), activation=\"relu\", trainable=False, bias=False, name=\"votes\")(sin_cos_vec) \n",
    "\n",
    "# A round about way of splitting the image (i.e vote array) into a bunch of non-overlapping cells of size (cell_size, cell_size)\n",
    "# then concateting values at each bin level, giving shape of (cell_nb, cell_nb, bins)\n",
    "# Result is an array of cells with histograms along the final axis\n",
    "cells = AveragePooling2D(pool_size=cell_size, strides=cell_size, name=\"cells\")(votes)\n",
    "cells = Lambda(lambda x: x * (cell_size**2), name=\"cells2\")(cells)\n",
    "\n",
    "# Bin Operations\n",
    "# Assuming that bin shape = (2, 2)\n",
    "# A round about way of grouping the cells into overlapping blocks of 2 * 2 cells each. \n",
    "# Two horizontally or vertically consecutive blocks overlap by two cells, that is, the block strides. \n",
    "# As a consequence, each internal cell is covered by four blocks (if bin_dim=2).\n",
    "bins1_layer = Lambda(bins1, trainable=False, name=\"bins1\")(cells)\n",
    "bins2_layer = Lambda(bins2, trainable=False, name=\"bins2\")(cells)\n",
    "bins3_layer = Lambda(bins3, trainable=False, name=\"bins3\")(cells)\n",
    "bins4_layer = Lambda(bins4, trainable=False, name=\"bins4\")(cells)\n",
    "bins_layer = Concatenate(axis=-1)([bins1_layer, bins2_layer, bins3_layer, bins4_layer])\n",
    "\n",
    "# normalize each block feature by its Euclidean norm\n",
    "bins_norm_layer = Lambda(bin_norm, name=\"norm1\")(bins_layer)\n",
    "hog_norm_layer = Lambda(hog_norm, name=\"norm2\")(bins_norm_layer)\n",
    "#bin_flatten_layer = Lambda(bins_flatten, name=\"flat\")(hog_norm_layer)  # Flatten Resulting array\n",
    "\n",
    "# Block 1\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1_')(hog_norm_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2_')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool_')(x)\n",
    "x = Dropout(0.05)(x)\n",
    "\n",
    "# Block 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1_')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2_')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool_')(x)\n",
    "x = Dropout(0.075)(x)\n",
    "\n",
    "\n",
    "# Block 3\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1_')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2_')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3_')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4_')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool_')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Block 4\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(1, 1), name='block4_pool')(x)\n",
    "x = Dropout(0.15)(x)\n",
    "\n",
    "# Block 5\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv4')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), strides=(1, 1), name='block5_pool')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Classification block\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(200, activation='relu', name='fc1')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(200, activation='relu', name='fc2_')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "logistic_reg = Dense(2, activation = \"softmax\", trainable=True, name=\"lr_\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=logistic_reg)\n",
    "model.load_weights('model_VGG_max.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('model_VGG_lite.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mags = Model(inputs=inputs, outputs=mags)\n",
    "model_angles = Model(inputs=inputs, outputs=angles)\n",
    "model_cells = Model(inputs=inputs, outputs=cells)\n",
    "#model_flatten = Model(inputs=inputs, outputs=bin_flatten_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.asarray(model.layers[9].get_weights()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[1].set_weights(prewitt_x)\n",
    "#model.layers[2].set_weights(prewitt_y)\n",
    "\n",
    "\n",
    "#model_mags.layers[1].set_weights(prewitt_x)\n",
    "#model_mags.layers[2].set_weights(prewitt_y)\n",
    "\n",
    "#model_angles.layers[1].set_weights(prewitt_x)\n",
    "#model_angles.layers[2].set_weights(prewitt_y)\n",
    "\n",
    "#model_cells.layers[1].set_weights(prewitt_x)\n",
    "#model_cells.layers[2].set_weights(prewitt_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#model.layers[9].set_weights(cent)\n",
    "#model_cells.layers[9].set_weights(cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nb = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mags = model_mags.predict(train_x[:batch_size, :, :, :], batch_size=batch_size)\n",
    "pred_angles = model_angles.predict(train_x[:batch_size, :, :, :], batch_size=batch_size)\n",
    "pred_cells = model_cells.predict(train_x[:batch_size, :, :, :], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred_mags[img_nb, :, :, 0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred_angles[img_nb, :, :])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pred_angles.max())\n",
    "print (pred_angles.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_nbs = 1\n",
    "plt.imshow(pred_cells[img_nb, :, :, bin_nbs])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print (pred_cells[img_nb, :, :, bin_nbs].max())\n",
    "print (pred_cells[img_nb, :, :, bin_nbs].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6100 samples, validate on 6100 samples\n",
      "Epoch 1/5\n",
      "6100/6100 [==============================] - 61s 10ms/step - loss: 0.1064 - acc: 0.9734 - val_loss: 0.8423 - val_acc: 0.9066\n",
      "Epoch 2/5\n",
      "6100/6100 [==============================] - 59s 10ms/step - loss: 0.0705 - acc: 0.9810 - val_loss: 2.1031 - val_acc: 0.8233\n",
      "Epoch 3/5\n",
      "6000/6100 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9890"
     ]
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "history = model.fit(train_x, conf_train_y, epochs=5, batch_size=batch_size, validation_data=(test_x, conf_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_VGG_max.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_VGG_max.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(model.layers[1].get_weights())[0, :, :, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(model.layers[2].get_weights())[0, :, :, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_HOG = model_flatten.predict(train_x, batch_size=batch_size)\n",
    "#test_HOG = model_flatten.predict(test_x, batch_size=batch_size)\n",
    "#print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "#clf = SGDClassifier(max_iter=1000)\n",
    "#clf = clf.fit(train_HOG, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score = clf.score(test_HOG, test_y)\n",
    "#score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
