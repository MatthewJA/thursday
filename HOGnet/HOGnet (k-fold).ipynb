{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduce a tensor-based implementation of the HOG feature extractor, built in Keras and Tensorflow. It is, to our knowledge, the first of its kind. To restrict all operations to the tensor domain, we approximate the previously employed histogram bin construction methodology, known as voting via bi-linear interpolation, as the scalar projection of each angle vector onto each separate bin unit vector. Since this is defined as a simple dot product between the two aforementioned vectors, we can implement this as a Convolution2D operation, with the angle vectors as the input and the bin unit vectors as the filters, with each filter representing a single unit vector that operates on every angle vector in the image. A Relu activation function operates on the output to curb the influence of negative projections. The reshaping required for the bin operations broken into separate depthwise convolution2D operations (via Tensorflow) followed via a Concatenation. \n",
    "\n",
    "As both Tensorflow allows the automatic computation of gradients, unlike the SkImage HOG, our Tensor-based HOG enables backpropagation through HOG. This opens up an array of previously unexplored possibilities. It is for instance now possible to have HOG as an intermediary layer, meaning that instead of having the raw images as inputs, we first pass them through a series of convolution layers and then into HOG, theoretically enabling the creation of a more powerful feature extractor. We shall test this premise in the not too distant future.\n",
    "\n",
    "For this notebook, we simply compare the performance of the outputs of the flattened bin array on both a keras dense logictic regiression and a Scipy logistic regression. We use centered samples. The results show that a simple scipy logictic regression on the output predictions prerforms significantly better than the intergrated keras Dense layer. Our current aim is to correct this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "#Importing packages\n",
    "import numpy as np\n",
    "import keras \n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, Input, Dense, Lambda, multiply, AveragePooling2D, Concatenate, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from keras.engine.topology import Layer\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import function\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "K.set_image_dim_ordering('tf')\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening pre-configured training and testing data.\n",
    "# Balanced classes and equal test/train split\n",
    "# Order: Samples are balanced, shuffled, split into training and testing sets, and then fed into the data generator separately.\n",
    "# Images are geneated through Keras' ImageDataGenerator, with the following settings:\n",
    "# rotation_range=180, zoom_range=0.2, horizontal_flip=True, vertical_flip =True, fill_mode = \"nearest\")\n",
    "\n",
    "file = open(\"train_y_fold.dat\",'rb')\n",
    "train_y_n = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"test_y_fold.dat\",'rb')\n",
    "test_y_n = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"train_x_fold.dat\",'rb')\n",
    "train_x_n = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"test_x_fold.dat\",'rb')\n",
    "test_x_n = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Values\n",
    "bins = 8         # number of bins in histogram\n",
    "w = 2*np.pi/bins     # width of each bin\n",
    "centers = np.arange(-np.pi, np.pi, w) + 0.5*w   #centers of each bin\n",
    "\n",
    "cell_size = 8   # Each cell will contain cell_size^2 pixels\n",
    "block_size = 2  # Each block will contain block_size^2 pixels\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train  = train_x_n.shape[0]\n",
    "nb_test  = test_x_n.shape[0]\n",
    "\n",
    "# Prosessing images to ensure there are no vanishing and exploding gradients\n",
    "train_x_n += (10e-10)*np.random.randn(nb_train, 256, 256, 1, 10)\n",
    "test_x_n += (10e-10)*np.random.randn(nb_test, 256, 256, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring samples are divisible by batch size\n",
    "if train_x_n.shape[0]%batch_size != 0:\n",
    "    \n",
    "    # Slicing off end of samples to ensure train_y.shape[0]%batch_size == 0\n",
    "    train_y = train_y_n[:-(train_y_n.shape[0]%batch_size)]\n",
    "    test_y = test_y_n[:-(test_y_n.shape[0]%batch_size)]\n",
    "    \n",
    "    # Slicing off end of samples to ensure train_x.shape[0]%batch_size == 0\n",
    "    # Adding dimention for the colour chanels (It's greyscale, hence only 1)\n",
    "    train_x = train_x_n.reshape((train_x_n.shape[0], train_x_n.shape[1], train_x_n.shape[2], 1, 10))[:-(train_x_n.shape[0]%batch_size), :, :, :, :]\n",
    "    test_x = test_x_n.reshape((test_x_n.shape[0], test_x_n.shape[1], test_x_n.shape[2], 1, 10))[:-(test_x_n.shape[0]%batch_size), :, :, :, :]\n",
    "\n",
    "# Special case (-0 indices returns zero images)\n",
    "elif train_x_n.shape[0]%batch_size == 0:\n",
    "    train_y = train_y_n\n",
    "    test_y = test_y_n\n",
    "    \n",
    "    train_x = train_x_n.reshape((train_x_n.shape[0], train_x_n.shape[1], train_x_n.shape[2], 1, 10))\n",
    "    test_x = test_x_n.reshape((test_x_n.shape[0], test_x_n.shape[1], test_x_n.shape[2], 1, 10))\n",
    "    \n",
    "else:\n",
    "    print (\"Biggly Error\")\n",
    "    \n",
    "conf_train_y  = np.zeros((train_x.shape[0], 2, 10))\n",
    "conf_test_y = np.zeros((test_x.shape[0], 2, 10))\n",
    "\n",
    "# Convert class vectors to binary class matrices (for Keras Dense)\n",
    "for fold in range(10):\n",
    "    conf_train_y[:, :, fold] = np_utils.to_categorical(train_y[:, fold], 2)\n",
    "    conf_test_y[:, :, fold] = np_utils.to_categorical(test_y[:, fold], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 2, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining Gradient functions to fix vanishing and exploding gradients\n",
    "@function.Defun(tf.float32, tf.float32)\n",
    "def norm_grad(x, dy):\n",
    "    return dy*(x/tf.norm(x))\n",
    "\n",
    "@function.Defun(tf.float32, grad_func=norm_grad)\n",
    "def norm(x):\n",
    "    return tf.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Weights for the vertical and horizontal convolutions to calculate the image gradients\n",
    "prewitt_x = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]).reshape((1, 3, 3, 1, 1)) + 0.01*np.random.randn(1, 3, 3, 1, 1)\n",
    "prewitt_y = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]).reshape((1, 3, 3, 1, 1)) + 0.01*np.random.randn(1, 3, 3, 1, 1)\n",
    "\n",
    "cent = np.vstack((np.sin(centers), np.cos(centers))).reshape((1, 1, 1, 2, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[-1.00840263]],\n",
       "\n",
       "         [[ 0.02803165]],\n",
       "\n",
       "         [[ 0.99479026]]],\n",
       "\n",
       "\n",
       "        [[[-0.99984235]],\n",
       "\n",
       "         [[ 0.00509896]],\n",
       "\n",
       "         [[ 1.00148176]]],\n",
       "\n",
       "\n",
       "        [[[-0.9955783 ]],\n",
       "\n",
       "         [[ 0.00320753]],\n",
       "\n",
       "         [[ 0.9960928 ]]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prewitt_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Filters for the Bin Operations\n",
    "def create_bin_filters(bin_dim):\n",
    "    filters = np.zeros((bin_dim**2, bin_dim, bin_dim))\n",
    "\n",
    "    count = 0 \n",
    "    for i in range(bin_dim):\n",
    "        for j in range(bin_dim):\n",
    "            filters[count, i, j] = 1\n",
    "            count += 1\n",
    "    return filters   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_dim must be 2\n",
    "# Increasing this will require the adding of some more tf.nn.depthwise_conv2d functions. There is one for each element in a single filter (i.e bin_dim^2)\n",
    "\n",
    "b_flt = create_bin_filters(2)\n",
    "bin_filters_n = b_flt.reshape((b_flt.shape[0], b_flt.shape[1], b_flt.shape[2], 1)).repeat(bins, 3) # Simply copying each filter anlong the last axis to satisfy the required shape for weights array (see Tensorflow docs for tf.nn.depthwise_conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping to satisfy required shape for weight array\n",
    "bin_filters = bin_filters_n.reshape(bin_filters_n.shape[0], bin_filters_n.shape[1], bin_filters_n.shape[2], bin_filters_n.shape[3], 1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_filters[0, :, :, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 3, 1, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prewitt_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_prewitt_x(shape, dtype=None):\n",
    "    shp = prewitt_x.shape\n",
    "    weight = K.variable(prewitt_x) + K.random_normal_variable(shp, mean=0.0, scale=0.001, dtype=dtype)\n",
    "    return weight\n",
    "\n",
    "def init_prewitt_y(shape, dtype=None):\n",
    "    shp = prewitt_y.shape\n",
    "    weight = K.variable(prewitt_y) + K.random_normal_variable(shp, mean=0.0, scale=0.001, dtype=dtype)\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model HyperParameters\n",
    "cell_dim = 8\n",
    "bin_dim = 2\n",
    "bin_stride_length = 1\n",
    "bs = bin_stride_length\n",
    "\n",
    "# Number of cells along each dim \n",
    "cell_nb = 128//cell_dim\n",
    "assert not 128 % cell_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt1 = tf.convert_to_tensor(bin_filters[0, :, :, :, :])\n",
    "filt2 = tf.convert_to_tensor(bin_filters[1, :, :, :, :])\n",
    "filt3 = tf.convert_to_tensor(bin_filters[2, :, :, :, :])\n",
    "filt4 = tf.convert_to_tensor(bin_filters[3, :, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef bins_flatten(bins):\\n    bins_flatten = tf.reshape(bins, (-1))\\n    return bins_flatten\\n\\ndef bin_flatten_output_shape(input_shape):\\n    shape = list(input_shape)\\n    assert len(shape) == 4  #only valid for 4D tensors\\n    shape = shape[0]*shape[1]*shape[2]*shape[3]\\n    return tuple(shape)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_magnitudes(conv_stacked):\n",
    "    #b_bound = 10.0e-10\n",
    "    #b_fill = tf.fill(tf.shape(conv_stacked), tf.constant(b_bound))\n",
    "    \n",
    "    #mag = tf.where(tf.abs(conv_stacked) < b_bound, b_fill, conv_stacked)\n",
    "    mags = tf.norm((conv_stacked), axis=3)\n",
    "    return mags\n",
    "\n",
    "def calculate_angles(conv_stacked):\n",
    "    angles = tf.atan2(conv_stacked[:, :, :, 1], conv_stacked[:, :, :, 0])\n",
    "    return angles\n",
    "\n",
    "def calculate_sin_cos(angles):\n",
    "    sin = K.sin(angles)\n",
    "    cos = K.cos(angles)\n",
    "    sin_cos = K.stack([sin, cos], axis =-1)\n",
    "    return sin_cos\n",
    "   \n",
    "def bins1(cells):\n",
    "    c_bins = tf.nn.depthwise_conv2d(cells, filt1, strides = (1, bs, bs, 1), padding=\"VALID\")\n",
    "    return c_bins\n",
    "\n",
    "def bins2(cells):\n",
    "    c_bins = tf.nn.depthwise_conv2d(cells, filt2, strides = (1, bs, bs, 1), padding=\"VALID\")\n",
    "    return c_bins \n",
    "\n",
    "def bins3(cells):\n",
    "    c_bins = tf.nn.depthwise_conv2d(cells, filt3, strides = (1, bs, bs, 1), padding=\"VALID\")\n",
    "    return c_bins\n",
    "\n",
    "def bins4(cells):\n",
    "    c_bins = tf.nn.depthwise_conv2d(cells, filt4, strides = (1, bs, bs, 1), padding=\"VALID\")\n",
    "    return c_bins\n",
    "\n",
    "def bin_norm(bins_layer):\n",
    "    bins_norm = tf.div(bins_layer, tf.expand_dims(tf.sqrt(tf.norm(bins_layer, axis=-1)+0.00000001), -1))\n",
    "    return bins_norm\n",
    "\n",
    "def hog_norm(bins_layer):\n",
    "    hog_norms = tf.div(bins_layer, tf.expand_dims(tf.sqrt(tf.norm(bins_layer, axis=-1)+0.00000001), -1))\n",
    "    hog_norms_2 = tf.div(hog_norms, tf.expand_dims(tf.sqrt(tf.norm(hog_norms, axis=-1)+0.00000001), -1))\n",
    "    return hog_norms_2\n",
    "\n",
    "def bins_flatten(bins_norm):\n",
    "    bins_flattened = tf.reshape(bins_norm, (batch_size, (cell_nb-bs)*(cell_nb-bs)*bins*(bin_dim**2)))\n",
    "    return bins_flattened\n",
    "\n",
    "def tensorflow_dense(bins_flattened):\n",
    "    logistic = tf.layers.dense(inputs=bins_flattened, units = 2, activation=\"sigmoid\")\n",
    "    return logistic\n",
    "\n",
    "\"\"\"\n",
    "def bins_flatten(bins):\n",
    "    bins_flatten = tf.reshape(bins, (-1))\n",
    "    return bins_flatten\n",
    "\n",
    "def bin_flatten_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 4  #only valid for 4D tensors\n",
    "    shape = shape[0]*shape[1]*shape[2]*shape[3]\n",
    "    return tuple(shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_shape = batch_size*(cell_nb-bs)*(cell_nb-bs)*bins*(bin_dim**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2200, 256, 256, 1, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, kernel_size=(1, 1), strides=(1, 1), activation=\"relu\", trainable=False, name=\"votes\", use_bias=False)`\n"
     ]
    }
   ],
   "source": [
    "# Building Model \n",
    "inputs = Input(shape = (256, 256, 1), name=\"input\")\n",
    "\n",
    "# Convolutions\n",
    "x_conv = Conv2D(1, (3,3), strides=1, padding=\"same\", data_format=\"channels_last\", trainable=False, use_bias=False, name=\"conv_x\")(inputs)\n",
    "y_conv = Conv2D(1, (3,3), strides=1, padding=\"same\", data_format=\"channels_last\", trainable=False, use_bias=False, name=\"conv_y\")(inputs)\n",
    "\n",
    "# Stacking since it appears you cannot have more than a single input into a layer (i.e mags and angles)\n",
    "conv_stacked = Concatenate(axis=-1, name=\"Conv_Stacked\")([x_conv, y_conv])\n",
    "\n",
    "# Calculating the gradient magnitudes and angles\n",
    "mags = Lambda(calculate_magnitudes, output_shape=(256, 256), name=\"mags1\")(conv_stacked)\n",
    "mags = Lambda(lambda x: K.stack((mags, mags), axis=-1), name=\"mags2\")(mags)   # To enable sin_cos_vec \n",
    "angles = Lambda(calculate_angles, output_shape=(256, 256), name=\"angles\")(conv_stacked)\n",
    "\n",
    "# Calculating the components of angles in the x and y direction\n",
    "# Then multiplying by magnitudes, giving angle vectors\n",
    "sin_cos = Lambda(calculate_sin_cos, output_shape=(256, 256, 2), name=\"sin_cos\")(angles)\n",
    "sin_cos_vec = multiply([sin_cos, mags], name=\"sin_cos_mag\")\n",
    "\n",
    "# Applying each filter (representing a single bin unit vector) to every angle vector in the image.\n",
    "# Result is an array with shape (img_height, img_width, bins) where each bin is contains an angle vectors contribution to each bin\n",
    "# Relu activation function to remove negative projections (represented by negative scalar dot products).\n",
    "votes = Conv2D(8, kernel_size=(1, 1), strides=(1, 1), activation=\"relu\", trainable=False, bias=False, name=\"votes\")(sin_cos_vec) \n",
    "\n",
    "# A round about way of splitting the image (i.e vote array) into a bunch of non-overlapping cells of size (cell_size, cell_size)\n",
    "# then concateting values at each bin level, giving shape of (cell_nb, cell_nb, bins)\n",
    "# Result is an array of cells with histograms along the final axis\n",
    "cells = AveragePooling2D(pool_size=cell_size, strides=cell_size, name=\"cells\")(votes)\n",
    "cells = Lambda(lambda x: x * (cell_size**2), name=\"cells2\")(cells)\n",
    "\n",
    "# Bin Operations\n",
    "# Assuming that bin shape = (2, 2)\n",
    "# A round about way of grouping the cells into overlapping blocks of 2 * 2 cells each. \n",
    "# Two horizontally or vertically consecutive blocks overlap by two cells, that is, the block strides. \n",
    "# As a consequence, each internal cell is covered by four blocks (if bin_dim=2).\n",
    "bins1_layer = Lambda(bins1, trainable=False, name=\"bins1\")(cells)\n",
    "bins2_layer = Lambda(bins2, trainable=False, name=\"bins2\")(cells)\n",
    "bins3_layer = Lambda(bins3, trainable=False, name=\"bins3\")(cells)\n",
    "bins4_layer = Lambda(bins4, trainable=False, name=\"bins4\")(cells)\n",
    "bins_layer = Concatenate(axis=-1)([bins1_layer, bins2_layer, bins3_layer, bins4_layer])\n",
    "\n",
    "# normalize each block feature by its Euclidean norm\n",
    "bins_norm_layer = Lambda(bin_norm, name=\"norm1\")(bins_layer)\n",
    "hog_norm_layer = Lambda(hog_norm, name=\"norm2\")(bins_norm_layer)\n",
    "#bin_flatten_layer = Lambda(bins_flatten, name=\"flat\")(hog_norm_layer)  # Flatten Resulting array\n",
    "\n",
    "# Block 1\n",
    "x = Conv2D(4, (3, 3), activation='relu', padding='same', name='block1_conv1')(hog_norm_layer)\n",
    "x = Conv2D(4, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = Conv2D(4, (3, 3), activation='relu', padding='same', name='block1_conv3')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "# Block 2\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same', name='block2_conv3')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "# Dense\n",
    "x = Flatten(name='flat')(x)\n",
    "x = Dense(50, activation='relu', name='fc1_')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(50, activation='relu', name='fc2_')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(50, activation='relu', name='fc3_')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "logistic_reg = Dense(2, activation = \"softmax\", trainable=True, name=\"lr_\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=logistic_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mags = Model(inputs=inputs, outputs=mags)\n",
    "model_angles = Model(inputs=inputs, outputs=angles)\n",
    "model_cells = Model(inputs=inputs, outputs=cells)\n",
    "#model_flatten = Model(inputs=inputs, outputs=bin_flatten_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.asarray(model.layers[9].get_weights()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[1].set_weights(prewitt_x)\n",
    "#model.layers[2].set_weights(prewitt_y)\n",
    "\n",
    "\n",
    "#model_mags.layers[1].set_weights(prewitt_x)\n",
    "#model_mags.layers[2].set_weights(prewitt_y)\n",
    "\n",
    "#model_angles.layers[1].set_weights(prewitt_x)\n",
    "#model_angles.layers[2].set_weights(prewitt_y)\n",
    "\n",
    "#model_cells.layers[1].set_weights(prewitt_x)\n",
    "#model_cells.layers[2].set_weights(prewitt_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('model_conv.h5')\n",
    "\n",
    "#model.layers[9].set_weights(cent)\n",
    "#model_cells.layers[9].set_weights(cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nb = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_mags = model_mags.predict(train_x[:batch_size, :, :, :, 0], batch_size=batch_size)\n",
    "#pred_angles = model_angles.predict(train_x[:batch_size, :, :, :, 0], batch_size=batch_size)\n",
    "#pred_cells = model_cells.predict(train_x[:batch_size, :, :, :, 0], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(pred_mags[img_nb, :, :, 0])\n",
    "#plt.colorbar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(pred_angles[img_nb, :, :])\n",
    "#plt.colorbar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (pred_angles.max())\n",
    "#print (pred_angles.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin_nbs = 5\n",
    "#plt.imshow(pred_cells[img_nb, :, :, bin_nbs])\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "\n",
    "#print (pred_cells[img_nb, :, :, bin_nbs].max())\n",
    "#print (pred_cells[img_nb, :, :, bin_nbs].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_x (Conv2D)                 (None, 256, 256, 1)  9           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_y (Conv2D)                 (None, 256, 256, 1)  9           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv_Stacked (Concatenate)      (None, 256, 256, 2)  0           conv_x[0][0]                     \n",
      "                                                                 conv_y[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "angles (Lambda)                 (None, 256, 256)     0           Conv_Stacked[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mags1 (Lambda)                  (None, 256, 256)     0           Conv_Stacked[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sin_cos (Lambda)                (None, 256, 256, 2)  0           angles[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mags2 (Lambda)                  (None, 256, 256, 2)  0           mags1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sin_cos_mag (Multiply)          (None, 256, 256, 2)  0           sin_cos[0][0]                    \n",
      "                                                                 mags2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "votes (Conv2D)                  (None, 256, 256, 8)  16          sin_cos_mag[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cells (AveragePooling2D)        (None, 32, 32, 8)    0           votes[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "cells2 (Lambda)                 (None, 32, 32, 8)    0           cells[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bins1 (Lambda)                  (None, 31, 31, 8)    0           cells2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bins2 (Lambda)                  (None, 31, 31, 8)    0           cells2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bins3 (Lambda)                  (None, 31, 31, 8)    0           cells2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bins4 (Lambda)                  (None, 31, 31, 8)    0           cells2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 31, 31, 32)   0           bins1[0][0]                      \n",
      "                                                                 bins2[0][0]                      \n",
      "                                                                 bins3[0][0]                      \n",
      "                                                                 bins4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "norm1 (Lambda)                  (None, 31, 31, 32)   0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm2 (Lambda)                  (None, 31, 31, 32)   0           norm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 31, 31, 4)    1156        norm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 31, 31, 4)    148         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv3 (Conv2D)           (None, 31, 31, 4)    148         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 15, 15, 4)    0           block1_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 15, 15, 8)    296         block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 15, 15, 8)    584         block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv3 (Conv2D)           (None, 15, 15, 8)    584         block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 7, 7, 8)      0           block2_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flat (Flatten)                  (None, 392)          0           block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "fc1_ (Dense)                    (None, 50)           19650       flat[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50)           0           fc1_[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc2_ (Dense)                    (None, 50)           2550        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 50)           0           fc2_[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc3_ (Dense)                    (None, 50)           2550        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 50)           0           fc3_[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lr_ (Dense)                     (None, 2)            102         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 27,802\n",
      "Trainable params: 27,768\n",
      "Non-trainable params: 34\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2200 samples, validate on 240 samples\n",
      "Epoch 1/60\n",
      "2200/2200 [==============================] - 6s 3ms/step - loss: 0.6935 - acc: 0.4950 - val_loss: 0.6926 - val_acc: 0.5000\n",
      "Epoch 2/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6921 - acc: 0.5095 - val_loss: 0.6860 - val_acc: 0.6125\n",
      "Epoch 3/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.5801 - acc: 0.7000 - val_loss: 0.4852 - val_acc: 0.7833\n",
      "Epoch 4/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.4393 - acc: 0.8186 - val_loss: 0.3747 - val_acc: 0.8458\n",
      "Epoch 5/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3722 - acc: 0.8545 - val_loss: 0.3913 - val_acc: 0.8583\n",
      "Epoch 6/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3312 - acc: 0.8655 - val_loss: 0.2860 - val_acc: 0.8833\n",
      "Epoch 7/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3029 - acc: 0.8777 - val_loss: 0.2990 - val_acc: 0.8833\n",
      "Epoch 8/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2461 - acc: 0.9086 - val_loss: 0.2524 - val_acc: 0.9042\n",
      "Epoch 9/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2285 - acc: 0.9086 - val_loss: 0.2677 - val_acc: 0.8875\n",
      "Epoch 10/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2000 - acc: 0.9227 - val_loss: 0.3140 - val_acc: 0.8833\n",
      "Epoch 11/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1748 - acc: 0.9318 - val_loss: 0.2184 - val_acc: 0.9167\n",
      "Epoch 12/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1684 - acc: 0.9373 - val_loss: 0.2226 - val_acc: 0.9500\n",
      "Epoch 13/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1525 - acc: 0.9368 - val_loss: 0.2699 - val_acc: 0.9167\n",
      "Epoch 14/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1446 - acc: 0.9527 - val_loss: 0.2221 - val_acc: 0.9292\n",
      "Epoch 15/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1290 - acc: 0.9550 - val_loss: 0.3296 - val_acc: 0.8875\n",
      "Epoch 16/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1151 - acc: 0.9545 - val_loss: 0.2426 - val_acc: 0.9417\n",
      "Epoch 17/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1062 - acc: 0.9623 - val_loss: 0.3347 - val_acc: 0.9250\n",
      "Epoch 18/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1068 - acc: 0.9618 - val_loss: 0.2951 - val_acc: 0.9208\n",
      "Epoch 19/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0750 - acc: 0.9741 - val_loss: 0.3077 - val_acc: 0.9250\n",
      "Epoch 20/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0824 - acc: 0.9700 - val_loss: 0.3235 - val_acc: 0.9083\n",
      "Epoch 21/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0736 - acc: 0.9764 - val_loss: 0.3234 - val_acc: 0.9333\n",
      "Epoch 22/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0675 - acc: 0.9827 - val_loss: 0.3335 - val_acc: 0.9292\n",
      "Epoch 23/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0610 - acc: 0.9782 - val_loss: 0.3590 - val_acc: 0.9292\n",
      "Epoch 24/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0662 - acc: 0.9800 - val_loss: 0.3818 - val_acc: 0.9333\n",
      "Epoch 25/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0537 - acc: 0.9814 - val_loss: 0.3382 - val_acc: 0.9333\n",
      "Epoch 26/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0615 - acc: 0.9809 - val_loss: 0.3396 - val_acc: 0.9125\n",
      "Epoch 27/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0481 - acc: 0.9864 - val_loss: 0.3287 - val_acc: 0.9417\n",
      "Epoch 28/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0674 - acc: 0.9768 - val_loss: 0.3406 - val_acc: 0.9333\n",
      "Epoch 29/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0492 - acc: 0.9859 - val_loss: 0.3449 - val_acc: 0.9375\n",
      "Epoch 30/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0459 - acc: 0.9845 - val_loss: 0.3426 - val_acc: 0.9458\n",
      "Epoch 31/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0522 - acc: 0.9845 - val_loss: 0.3413 - val_acc: 0.9250\n",
      "Epoch 32/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0420 - acc: 0.9891 - val_loss: 0.3509 - val_acc: 0.9250\n",
      "Epoch 33/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0446 - acc: 0.9886 - val_loss: 0.3891 - val_acc: 0.9458\n",
      "Epoch 34/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0368 - acc: 0.9882 - val_loss: 0.3778 - val_acc: 0.9375\n",
      "Epoch 35/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0512 - acc: 0.9832 - val_loss: 0.4621 - val_acc: 0.9208\n",
      "Epoch 36/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0465 - acc: 0.9859 - val_loss: 0.4637 - val_acc: 0.9042\n",
      "Epoch 37/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0258 - acc: 0.9895 - val_loss: 0.4176 - val_acc: 0.9375\n",
      "Epoch 38/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0245 - acc: 0.9918 - val_loss: 0.5836 - val_acc: 0.9042\n",
      "Epoch 39/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0354 - acc: 0.9868 - val_loss: 0.4630 - val_acc: 0.9208\n",
      "Epoch 40/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0355 - acc: 0.9882 - val_loss: 0.4355 - val_acc: 0.9333\n",
      "Epoch 41/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0255 - acc: 0.9918 - val_loss: 0.4456 - val_acc: 0.9333\n",
      "Epoch 42/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0465 - acc: 0.9859 - val_loss: 0.3764 - val_acc: 0.9500\n",
      "Epoch 43/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0338 - acc: 0.9895 - val_loss: 0.3560 - val_acc: 0.9292\n",
      "Epoch 44/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0203 - acc: 0.9945 - val_loss: 0.3987 - val_acc: 0.9375\n",
      "Epoch 45/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0297 - acc: 0.9900 - val_loss: 0.4787 - val_acc: 0.9083\n",
      "Epoch 46/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0265 - acc: 0.9909 - val_loss: 0.4470 - val_acc: 0.9375\n",
      "Epoch 47/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0353 - acc: 0.9864 - val_loss: 0.4028 - val_acc: 0.9333\n",
      "Epoch 48/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0236 - acc: 0.9932 - val_loss: 0.5150 - val_acc: 0.9292\n",
      "Epoch 49/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0334 - acc: 0.9882 - val_loss: 0.4825 - val_acc: 0.9167\n",
      "Epoch 50/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0160 - acc: 0.9941 - val_loss: 0.4019 - val_acc: 0.9458\n",
      "Epoch 51/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0350 - acc: 0.9895 - val_loss: 0.4262 - val_acc: 0.9292\n",
      "Epoch 52/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0236 - acc: 0.9927 - val_loss: 0.4007 - val_acc: 0.9250\n",
      "Epoch 53/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0152 - acc: 0.9955 - val_loss: 0.4279 - val_acc: 0.9417\n",
      "Epoch 54/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0262 - acc: 0.9932 - val_loss: 0.5409 - val_acc: 0.9292\n",
      "Epoch 55/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0445 - acc: 0.9850 - val_loss: 0.3889 - val_acc: 0.9333\n",
      "Epoch 56/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0112 - acc: 0.9977 - val_loss: 0.4463 - val_acc: 0.9417\n",
      "Epoch 57/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0201 - acc: 0.9941 - val_loss: 0.5259 - val_acc: 0.9250\n",
      "Epoch 58/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0364 - acc: 0.9873 - val_loss: 0.3938 - val_acc: 0.9292\n",
      "Epoch 59/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0147 - acc: 0.9968 - val_loss: 0.5379 - val_acc: 0.9333\n",
      "Epoch 60/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0378 - acc: 0.9895 - val_loss: 0.3669 - val_acc: 0.9125\n",
      "Train on 2200 samples, validate on 240 samples\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6938 - acc: 0.4905 - val_loss: 0.6920 - val_acc: 0.5000\n",
      "Epoch 2/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6920 - acc: 0.5177 - val_loss: 0.6916 - val_acc: 0.5000\n",
      "Epoch 3/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6847 - acc: 0.5564 - val_loss: 0.6697 - val_acc: 0.6708\n",
      "Epoch 4/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6773 - acc: 0.5650 - val_loss: 0.6458 - val_acc: 0.6583\n",
      "Epoch 5/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6835 - acc: 0.5345 - val_loss: 0.6911 - val_acc: 0.5000\n",
      "Epoch 6/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6729 - acc: 0.5786 - val_loss: 0.6455 - val_acc: 0.6250\n",
      "Epoch 7/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6446 - acc: 0.6227 - val_loss: 0.6307 - val_acc: 0.6417\n",
      "Epoch 8/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6147 - acc: 0.6691 - val_loss: 0.6021 - val_acc: 0.6667\n",
      "Epoch 9/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.5809 - acc: 0.7014 - val_loss: 0.5661 - val_acc: 0.6875\n",
      "Epoch 10/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.5381 - acc: 0.7364 - val_loss: 0.5222 - val_acc: 0.7292\n",
      "Epoch 11/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.5097 - acc: 0.7541 - val_loss: 0.4598 - val_acc: 0.7792\n",
      "Epoch 12/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.4780 - acc: 0.7845 - val_loss: 0.4614 - val_acc: 0.7792\n",
      "Epoch 13/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.4308 - acc: 0.8109 - val_loss: 0.4022 - val_acc: 0.8083\n",
      "Epoch 14/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3814 - acc: 0.8355 - val_loss: 0.3837 - val_acc: 0.8167\n",
      "Epoch 15/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3767 - acc: 0.8436 - val_loss: 0.3839 - val_acc: 0.8083\n",
      "Epoch 16/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3407 - acc: 0.8591 - val_loss: 0.3464 - val_acc: 0.8375\n",
      "Epoch 17/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3141 - acc: 0.8777 - val_loss: 0.3755 - val_acc: 0.8333\n",
      "Epoch 18/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3007 - acc: 0.8823 - val_loss: 0.3594 - val_acc: 0.8375\n",
      "Epoch 19/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2740 - acc: 0.8964 - val_loss: 0.3604 - val_acc: 0.8375\n",
      "Epoch 20/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2547 - acc: 0.9036 - val_loss: 0.3167 - val_acc: 0.8708\n",
      "Epoch 21/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2310 - acc: 0.9100 - val_loss: 0.3394 - val_acc: 0.8542\n",
      "Epoch 22/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2502 - acc: 0.9036 - val_loss: 0.3252 - val_acc: 0.8667\n",
      "Epoch 23/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2059 - acc: 0.9255 - val_loss: 0.3092 - val_acc: 0.8792\n",
      "Epoch 24/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1904 - acc: 0.9291 - val_loss: 0.3083 - val_acc: 0.8667\n",
      "Epoch 25/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1656 - acc: 0.9373 - val_loss: 0.3388 - val_acc: 0.8708\n",
      "Epoch 26/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1670 - acc: 0.9350 - val_loss: 0.3668 - val_acc: 0.8375\n",
      "Epoch 27/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1686 - acc: 0.9355 - val_loss: 0.3533 - val_acc: 0.8750\n",
      "Epoch 28/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1385 - acc: 0.9477 - val_loss: 0.3971 - val_acc: 0.8833\n",
      "Epoch 29/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1272 - acc: 0.9518 - val_loss: 0.4076 - val_acc: 0.8667\n",
      "Epoch 30/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1143 - acc: 0.9609 - val_loss: 0.4437 - val_acc: 0.8458\n",
      "Epoch 31/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0843 - acc: 0.9705 - val_loss: 0.3920 - val_acc: 0.8875\n",
      "Epoch 32/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1038 - acc: 0.9595 - val_loss: 0.4970 - val_acc: 0.8375\n",
      "Epoch 33/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0904 - acc: 0.9632 - val_loss: 0.4634 - val_acc: 0.8625\n",
      "Epoch 34/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0720 - acc: 0.9745 - val_loss: 0.4929 - val_acc: 0.8667\n",
      "Epoch 35/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0892 - acc: 0.9668 - val_loss: 0.4652 - val_acc: 0.8667\n",
      "Epoch 36/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0923 - acc: 0.9677 - val_loss: 0.4766 - val_acc: 0.8625\n",
      "Epoch 37/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0507 - acc: 0.9855 - val_loss: 0.4545 - val_acc: 0.8833\n",
      "Epoch 38/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0617 - acc: 0.9750 - val_loss: 0.4794 - val_acc: 0.8667\n",
      "Epoch 39/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0648 - acc: 0.9791 - val_loss: 0.4965 - val_acc: 0.8542\n",
      "Epoch 40/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0629 - acc: 0.9773 - val_loss: 0.5460 - val_acc: 0.8667\n",
      "Epoch 41/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0454 - acc: 0.9841 - val_loss: 0.5501 - val_acc: 0.8750\n",
      "Epoch 42/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0430 - acc: 0.9827 - val_loss: 0.4690 - val_acc: 0.8917\n",
      "Epoch 43/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0693 - acc: 0.9732 - val_loss: 0.4633 - val_acc: 0.8708\n",
      "Epoch 44/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0338 - acc: 0.9905 - val_loss: 0.7778 - val_acc: 0.8542\n",
      "Epoch 45/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0540 - acc: 0.9805 - val_loss: 0.5697 - val_acc: 0.8625\n",
      "Epoch 46/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0458 - acc: 0.9827 - val_loss: 0.5657 - val_acc: 0.8875\n",
      "Epoch 47/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0393 - acc: 0.9868 - val_loss: 0.6353 - val_acc: 0.8708\n",
      "Epoch 48/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0465 - acc: 0.9818 - val_loss: 0.6225 - val_acc: 0.8792\n",
      "Epoch 49/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0305 - acc: 0.9905 - val_loss: 0.7697 - val_acc: 0.8667\n",
      "Epoch 50/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0622 - acc: 0.9741 - val_loss: 0.4825 - val_acc: 0.8958\n",
      "Epoch 51/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0271 - acc: 0.9923 - val_loss: 0.4589 - val_acc: 0.9000\n",
      "Epoch 52/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0259 - acc: 0.9905 - val_loss: 0.6320 - val_acc: 0.8792\n",
      "Epoch 53/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0467 - acc: 0.9886 - val_loss: 0.6148 - val_acc: 0.8500\n",
      "Epoch 54/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0339 - acc: 0.9841 - val_loss: 0.4531 - val_acc: 0.8917\n",
      "Epoch 55/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0396 - acc: 0.9841 - val_loss: 0.5357 - val_acc: 0.8875\n",
      "Epoch 56/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0314 - acc: 0.9891 - val_loss: 0.6387 - val_acc: 0.8708\n",
      "Epoch 57/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0404 - acc: 0.9882 - val_loss: 0.5525 - val_acc: 0.8792\n",
      "Epoch 58/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0222 - acc: 0.9932 - val_loss: 0.6529 - val_acc: 0.8792\n",
      "Epoch 59/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0410 - acc: 0.9900 - val_loss: 0.5989 - val_acc: 0.8750\n",
      "Epoch 60/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0225 - acc: 0.9950 - val_loss: 0.6384 - val_acc: 0.8875\n",
      "Train on 2200 samples, validate on 240 samples\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6934 - acc: 0.4991 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 2/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6930 - acc: 0.5050 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 3/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6937 - acc: 0.4923 - val_loss: 0.6916 - val_acc: 0.5500\n",
      "Epoch 4/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6925 - acc: 0.5127 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 5/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6934 - acc: 0.5009 - val_loss: 0.6928 - val_acc: 0.5042\n",
      "Epoch 6/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6906 - acc: 0.5295 - val_loss: 0.6874 - val_acc: 0.5833\n",
      "Epoch 7/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6764 - acc: 0.5818 - val_loss: 0.6571 - val_acc: 0.6333\n",
      "Epoch 8/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6448 - acc: 0.6268 - val_loss: 0.6079 - val_acc: 0.6625\n",
      "Epoch 9/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6289 - acc: 0.6505 - val_loss: 0.6085 - val_acc: 0.6792\n",
      "Epoch 10/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.5785 - acc: 0.7105 - val_loss: 0.5416 - val_acc: 0.7042\n",
      "Epoch 11/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.5390 - acc: 0.7250 - val_loss: 0.5522 - val_acc: 0.7542\n",
      "Epoch 12/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.5105 - acc: 0.7568 - val_loss: 0.5053 - val_acc: 0.7875\n",
      "Epoch 13/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.4873 - acc: 0.7809 - val_loss: 0.4573 - val_acc: 0.7958\n",
      "Epoch 14/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.4376 - acc: 0.8109 - val_loss: 0.4396 - val_acc: 0.7917\n",
      "Epoch 15/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.4091 - acc: 0.8268 - val_loss: 0.4569 - val_acc: 0.7958\n",
      "Epoch 16/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3724 - acc: 0.8486 - val_loss: 0.4226 - val_acc: 0.8250\n",
      "Epoch 17/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3587 - acc: 0.8586 - val_loss: 0.3955 - val_acc: 0.8250\n",
      "Epoch 18/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3226 - acc: 0.8741 - val_loss: 0.3904 - val_acc: 0.8167\n",
      "Epoch 19/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2729 - acc: 0.8918 - val_loss: 0.3649 - val_acc: 0.8500\n",
      "Epoch 20/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2702 - acc: 0.8968 - val_loss: 0.3949 - val_acc: 0.8208\n",
      "Epoch 21/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2575 - acc: 0.9032 - val_loss: 0.3949 - val_acc: 0.8458\n",
      "Epoch 22/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2120 - acc: 0.9164 - val_loss: 0.3890 - val_acc: 0.8583\n",
      "Epoch 23/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2276 - acc: 0.9127 - val_loss: 0.4015 - val_acc: 0.8417\n",
      "Epoch 24/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1954 - acc: 0.9273 - val_loss: 0.3959 - val_acc: 0.8333\n",
      "Epoch 25/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1800 - acc: 0.9323 - val_loss: 0.4111 - val_acc: 0.8417\n",
      "Epoch 26/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1504 - acc: 0.9414 - val_loss: 0.3122 - val_acc: 0.8750\n",
      "Epoch 27/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1485 - acc: 0.9459 - val_loss: 0.3755 - val_acc: 0.8583\n",
      "Epoch 28/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1260 - acc: 0.9555 - val_loss: 0.3546 - val_acc: 0.8500\n",
      "Epoch 29/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1172 - acc: 0.9573 - val_loss: 0.4385 - val_acc: 0.8375\n",
      "Epoch 30/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1144 - acc: 0.9582 - val_loss: 0.3844 - val_acc: 0.8875\n",
      "Epoch 31/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1112 - acc: 0.9595 - val_loss: 0.4237 - val_acc: 0.8833\n",
      "Epoch 32/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1030 - acc: 0.9655 - val_loss: 0.5032 - val_acc: 0.8667\n",
      "Epoch 33/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0910 - acc: 0.9664 - val_loss: 0.6878 - val_acc: 0.8250\n",
      "Epoch 34/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0983 - acc: 0.9632 - val_loss: 0.4220 - val_acc: 0.8583\n",
      "Epoch 35/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0772 - acc: 0.9745 - val_loss: 0.5260 - val_acc: 0.8542\n",
      "Epoch 36/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0664 - acc: 0.9786 - val_loss: 0.4459 - val_acc: 0.8750\n",
      "Epoch 37/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0905 - acc: 0.9664 - val_loss: 0.5692 - val_acc: 0.8625\n",
      "Epoch 38/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0541 - acc: 0.9814 - val_loss: 0.5051 - val_acc: 0.8667\n",
      "Epoch 39/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0532 - acc: 0.9786 - val_loss: 0.5907 - val_acc: 0.8875\n",
      "Epoch 40/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0848 - acc: 0.9655 - val_loss: 0.6387 - val_acc: 0.8500\n",
      "Epoch 41/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0489 - acc: 0.9818 - val_loss: 0.5640 - val_acc: 0.8833\n",
      "Epoch 42/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0603 - acc: 0.9768 - val_loss: 0.6795 - val_acc: 0.8542\n",
      "Epoch 43/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0323 - acc: 0.9895 - val_loss: 0.6012 - val_acc: 0.8625\n",
      "Epoch 44/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0272 - acc: 0.9918 - val_loss: 0.6932 - val_acc: 0.8792\n",
      "Epoch 45/60\n",
      "2200/2200 [==============================] - 6s 3ms/step - loss: 0.1000 - acc: 0.9655 - val_loss: 0.4355 - val_acc: 0.8833\n",
      "Epoch 46/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0315 - acc: 0.9900 - val_loss: 0.5783 - val_acc: 0.8792\n",
      "Epoch 47/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0323 - acc: 0.9891 - val_loss: 0.6258 - val_acc: 0.8708\n",
      "Epoch 48/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0219 - acc: 0.9923 - val_loss: 0.7029 - val_acc: 0.8625\n",
      "Epoch 49/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0360 - acc: 0.9895 - val_loss: 0.7703 - val_acc: 0.8542\n",
      "Epoch 50/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0330 - acc: 0.9900 - val_loss: 0.6419 - val_acc: 0.8708\n",
      "Epoch 51/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0500 - acc: 0.9859 - val_loss: 0.5353 - val_acc: 0.8833\n",
      "Epoch 52/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0322 - acc: 0.9918 - val_loss: 0.7010 - val_acc: 0.8583\n",
      "Epoch 53/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0364 - acc: 0.9873 - val_loss: 0.8546 - val_acc: 0.8458\n",
      "Epoch 54/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0274 - acc: 0.9914 - val_loss: 0.6993 - val_acc: 0.8708\n",
      "Epoch 55/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0404 - acc: 0.9850 - val_loss: 0.5523 - val_acc: 0.8917\n",
      "Epoch 56/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0262 - acc: 0.9909 - val_loss: 0.5638 - val_acc: 0.8750\n",
      "Epoch 57/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0156 - acc: 0.9945 - val_loss: 0.9121 - val_acc: 0.8375\n",
      "Epoch 58/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0241 - acc: 0.9927 - val_loss: 0.8726 - val_acc: 0.8542\n",
      "Epoch 59/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0539 - acc: 0.9814 - val_loss: 0.6947 - val_acc: 0.8667\n",
      "Epoch 60/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0115 - acc: 0.9968 - val_loss: 0.9234 - val_acc: 0.8625\n",
      "Train on 2200 samples, validate on 240 samples\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6935 - acc: 0.4700 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 2/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6935 - acc: 0.4936 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 3/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6929 - acc: 0.5050 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 4/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6929 - acc: 0.5109 - val_loss: 0.6928 - val_acc: 0.5000\n",
      "Epoch 5/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6927 - acc: 0.5009 - val_loss: 0.6921 - val_acc: 0.5000\n",
      "Epoch 6/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6915 - acc: 0.5209 - val_loss: 0.6908 - val_acc: 0.5250\n",
      "Epoch 7/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6875 - acc: 0.5550 - val_loss: 0.6948 - val_acc: 0.5000\n",
      "Epoch 8/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6832 - acc: 0.5632 - val_loss: 0.6707 - val_acc: 0.6208\n",
      "Epoch 9/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6722 - acc: 0.5841 - val_loss: 0.6607 - val_acc: 0.6083\n",
      "Epoch 10/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6806 - acc: 0.5605 - val_loss: 0.6766 - val_acc: 0.6083\n",
      "Epoch 11/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6550 - acc: 0.6095 - val_loss: 0.7078 - val_acc: 0.5000\n",
      "Epoch 12/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6430 - acc: 0.6195 - val_loss: 0.6134 - val_acc: 0.6583\n",
      "Epoch 13/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.5934 - acc: 0.6950 - val_loss: 0.5805 - val_acc: 0.6625\n",
      "Epoch 14/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.5410 - acc: 0.7327 - val_loss: 0.5345 - val_acc: 0.7250\n",
      "Epoch 15/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.4808 - acc: 0.7777 - val_loss: 0.5067 - val_acc: 0.7458\n",
      "Epoch 16/60\n",
      "2200/2200 [==============================] - 6s 3ms/step - loss: 0.4538 - acc: 0.7995 - val_loss: 0.4736 - val_acc: 0.7708\n",
      "Epoch 17/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3904 - acc: 0.8359 - val_loss: 0.4742 - val_acc: 0.7583\n",
      "Epoch 18/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3691 - acc: 0.8468 - val_loss: 0.4461 - val_acc: 0.7917\n",
      "Epoch 19/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3256 - acc: 0.8686 - val_loss: 0.4448 - val_acc: 0.8167\n",
      "Epoch 20/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3213 - acc: 0.8718 - val_loss: 0.3816 - val_acc: 0.8375\n",
      "Epoch 21/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2805 - acc: 0.8941 - val_loss: 0.4013 - val_acc: 0.8375\n",
      "Epoch 22/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2590 - acc: 0.8982 - val_loss: 0.3860 - val_acc: 0.8250\n",
      "Epoch 23/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2283 - acc: 0.9105 - val_loss: 0.3663 - val_acc: 0.8458\n",
      "Epoch 24/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2128 - acc: 0.9159 - val_loss: 0.3483 - val_acc: 0.8708\n",
      "Epoch 25/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1727 - acc: 0.9345 - val_loss: 0.4520 - val_acc: 0.8417\n",
      "Epoch 26/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1891 - acc: 0.9309 - val_loss: 0.3328 - val_acc: 0.8792\n",
      "Epoch 27/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1635 - acc: 0.9405 - val_loss: 0.3740 - val_acc: 0.8667\n",
      "Epoch 28/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1269 - acc: 0.9532 - val_loss: 0.4310 - val_acc: 0.8667\n",
      "Epoch 29/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1136 - acc: 0.9582 - val_loss: 0.3876 - val_acc: 0.8458\n",
      "Epoch 30/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1098 - acc: 0.9577 - val_loss: 0.4621 - val_acc: 0.8625\n",
      "Epoch 31/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1042 - acc: 0.9636 - val_loss: 0.3844 - val_acc: 0.8708\n",
      "Epoch 32/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0979 - acc: 0.9645 - val_loss: 0.3568 - val_acc: 0.8750\n",
      "Epoch 33/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0864 - acc: 0.9673 - val_loss: 0.4378 - val_acc: 0.8500\n",
      "Epoch 34/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0747 - acc: 0.9732 - val_loss: 0.4552 - val_acc: 0.8917\n",
      "Epoch 35/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0696 - acc: 0.9736 - val_loss: 0.6065 - val_acc: 0.8792\n",
      "Epoch 36/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0678 - acc: 0.9736 - val_loss: 0.5262 - val_acc: 0.8625\n",
      "Epoch 37/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0633 - acc: 0.9777 - val_loss: 0.5319 - val_acc: 0.8667\n",
      "Epoch 38/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0683 - acc: 0.9741 - val_loss: 0.4908 - val_acc: 0.8375\n",
      "Epoch 39/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0484 - acc: 0.9836 - val_loss: 0.3923 - val_acc: 0.8875\n",
      "Epoch 40/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0641 - acc: 0.9768 - val_loss: 0.5222 - val_acc: 0.8750\n",
      "Epoch 41/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0322 - acc: 0.9891 - val_loss: 0.8076 - val_acc: 0.8333\n",
      "Epoch 42/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0669 - acc: 0.9768 - val_loss: 0.5325 - val_acc: 0.8625\n",
      "Epoch 43/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0499 - acc: 0.9791 - val_loss: 0.6798 - val_acc: 0.8625\n",
      "Epoch 44/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0306 - acc: 0.9877 - val_loss: 0.5156 - val_acc: 0.8792\n",
      "Epoch 45/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0411 - acc: 0.9868 - val_loss: 0.5613 - val_acc: 0.8625\n",
      "Epoch 46/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0295 - acc: 0.9895 - val_loss: 0.6033 - val_acc: 0.8667\n",
      "Epoch 47/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0269 - acc: 0.9909 - val_loss: 0.6089 - val_acc: 0.8917\n",
      "Epoch 48/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0272 - acc: 0.9895 - val_loss: 0.9099 - val_acc: 0.8542\n",
      "Epoch 49/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0679 - acc: 0.9795 - val_loss: 0.5539 - val_acc: 0.8625\n",
      "Epoch 50/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0509 - acc: 0.9836 - val_loss: 0.4421 - val_acc: 0.9042\n",
      "Epoch 51/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0251 - acc: 0.9905 - val_loss: 0.6909 - val_acc: 0.8542\n",
      "Epoch 52/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0357 - acc: 0.9886 - val_loss: 0.6470 - val_acc: 0.8333\n",
      "Epoch 53/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0483 - acc: 0.9827 - val_loss: 0.4379 - val_acc: 0.8833\n",
      "Epoch 54/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0240 - acc: 0.9918 - val_loss: 0.6068 - val_acc: 0.8917\n",
      "Epoch 55/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0218 - acc: 0.9918 - val_loss: 0.6403 - val_acc: 0.8750\n",
      "Epoch 56/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0235 - acc: 0.9918 - val_loss: 0.6846 - val_acc: 0.8667\n",
      "Epoch 57/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0333 - acc: 0.9891 - val_loss: 0.4917 - val_acc: 0.8792\n",
      "Epoch 58/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0451 - acc: 0.9832 - val_loss: 0.4602 - val_acc: 0.9000\n",
      "Epoch 59/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0156 - acc: 0.9932 - val_loss: 0.6811 - val_acc: 0.8708\n",
      "Epoch 60/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0219 - acc: 0.9918 - val_loss: 0.6335 - val_acc: 0.8917\n",
      "Train on 2200 samples, validate on 240 samples\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6939 - acc: 0.5000 - val_loss: 0.6925 - val_acc: 0.5000\n",
      "Epoch 2/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6934 - acc: 0.4918 - val_loss: 0.6928 - val_acc: 0.5000\n",
      "Epoch 3/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6932 - acc: 0.5118 - val_loss: 0.6924 - val_acc: 0.5000\n",
      "Epoch 4/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6930 - acc: 0.4991 - val_loss: 0.6925 - val_acc: 0.5000\n",
      "Epoch 5/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6931 - acc: 0.5114 - val_loss: 0.6918 - val_acc: 0.5000\n",
      "Epoch 6/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6937 - acc: 0.5009 - val_loss: 0.6922 - val_acc: 0.5000\n",
      "Epoch 7/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6924 - acc: 0.5118 - val_loss: 0.6922 - val_acc: 0.5000\n",
      "Epoch 8/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6930 - acc: 0.5205 - val_loss: 0.6895 - val_acc: 0.6208\n",
      "Epoch 9/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6906 - acc: 0.5295 - val_loss: 0.6858 - val_acc: 0.5917\n",
      "Epoch 10/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6898 - acc: 0.5341 - val_loss: 0.6850 - val_acc: 0.5042\n",
      "Epoch 11/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6761 - acc: 0.5714 - val_loss: 0.6411 - val_acc: 0.6250\n",
      "Epoch 12/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6580 - acc: 0.6195 - val_loss: 0.6096 - val_acc: 0.7333\n",
      "Epoch 13/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6237 - acc: 0.6518 - val_loss: 0.5410 - val_acc: 0.7458\n",
      "Epoch 14/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6101 - acc: 0.6636 - val_loss: 0.5082 - val_acc: 0.7583\n",
      "Epoch 15/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.5307 - acc: 0.7350 - val_loss: 0.4776 - val_acc: 0.8000\n",
      "Epoch 16/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.4779 - acc: 0.7841 - val_loss: 0.4435 - val_acc: 0.8083\n",
      "Epoch 17/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.4417 - acc: 0.8032 - val_loss: 0.4522 - val_acc: 0.7833\n",
      "Epoch 18/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.4244 - acc: 0.8123 - val_loss: 0.4295 - val_acc: 0.8042\n",
      "Epoch 19/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3942 - acc: 0.8382 - val_loss: 0.4118 - val_acc: 0.8125\n",
      "Epoch 20/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3608 - acc: 0.8536 - val_loss: 0.3955 - val_acc: 0.8292\n",
      "Epoch 21/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3281 - acc: 0.8641 - val_loss: 0.4185 - val_acc: 0.8250\n",
      "Epoch 22/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.3043 - acc: 0.8764 - val_loss: 0.3859 - val_acc: 0.8375\n",
      "Epoch 23/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2763 - acc: 0.8905 - val_loss: 0.4325 - val_acc: 0.8292\n",
      "Epoch 24/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2415 - acc: 0.9059 - val_loss: 0.4603 - val_acc: 0.8167\n",
      "Epoch 25/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2234 - acc: 0.9168 - val_loss: 0.4246 - val_acc: 0.8292\n",
      "Epoch 26/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.2037 - acc: 0.9182 - val_loss: 0.4771 - val_acc: 0.8208\n",
      "Epoch 27/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1975 - acc: 0.9314 - val_loss: 0.4077 - val_acc: 0.8542\n",
      "Epoch 28/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1487 - acc: 0.9459 - val_loss: 0.4775 - val_acc: 0.8667\n",
      "Epoch 29/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1572 - acc: 0.9418 - val_loss: 0.5213 - val_acc: 0.8250\n",
      "Epoch 30/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1270 - acc: 0.9527 - val_loss: 0.4806 - val_acc: 0.8167\n",
      "Epoch 31/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1329 - acc: 0.9536 - val_loss: 0.4369 - val_acc: 0.8292\n",
      "Epoch 32/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1199 - acc: 0.9568 - val_loss: 0.4696 - val_acc: 0.8167\n",
      "Epoch 33/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0982 - acc: 0.9691 - val_loss: 0.4979 - val_acc: 0.8375\n",
      "Epoch 34/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1111 - acc: 0.9586 - val_loss: 0.5239 - val_acc: 0.8292\n",
      "Epoch 35/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.1014 - acc: 0.9609 - val_loss: 0.5132 - val_acc: 0.8625\n",
      "Epoch 36/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0654 - acc: 0.9800 - val_loss: 0.6668 - val_acc: 0.8375\n",
      "Epoch 37/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0689 - acc: 0.9750 - val_loss: 0.6682 - val_acc: 0.8250\n",
      "Epoch 38/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0630 - acc: 0.9795 - val_loss: 0.6505 - val_acc: 0.8417\n",
      "Epoch 39/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0603 - acc: 0.9805 - val_loss: 0.7251 - val_acc: 0.8167\n",
      "Epoch 40/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0637 - acc: 0.9777 - val_loss: 0.5146 - val_acc: 0.8208\n",
      "Epoch 41/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0495 - acc: 0.9836 - val_loss: 0.6639 - val_acc: 0.8333\n",
      "Epoch 42/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0604 - acc: 0.9786 - val_loss: 0.6046 - val_acc: 0.8542\n",
      "Epoch 43/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0595 - acc: 0.9800 - val_loss: 0.6290 - val_acc: 0.8333\n",
      "Epoch 44/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0358 - acc: 0.9882 - val_loss: 0.8122 - val_acc: 0.8208\n",
      "Epoch 45/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0502 - acc: 0.9836 - val_loss: 0.6188 - val_acc: 0.8250\n",
      "Epoch 46/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0294 - acc: 0.9918 - val_loss: 0.9388 - val_acc: 0.8292\n",
      "Epoch 47/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0503 - acc: 0.9836 - val_loss: 0.5928 - val_acc: 0.8542\n",
      "Epoch 48/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0483 - acc: 0.9836 - val_loss: 0.6858 - val_acc: 0.8458\n",
      "Epoch 49/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0271 - acc: 0.9909 - val_loss: 0.7315 - val_acc: 0.8458\n",
      "Epoch 50/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0550 - acc: 0.9845 - val_loss: 0.7670 - val_acc: 0.8375\n",
      "Epoch 51/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0387 - acc: 0.9845 - val_loss: 0.6984 - val_acc: 0.8583\n",
      "Epoch 52/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0234 - acc: 0.9927 - val_loss: 0.7529 - val_acc: 0.8792\n",
      "Epoch 53/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0518 - acc: 0.9818 - val_loss: 0.6405 - val_acc: 0.8542\n",
      "Epoch 54/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0289 - acc: 0.9886 - val_loss: 0.8944 - val_acc: 0.8208\n",
      "Epoch 55/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0253 - acc: 0.9923 - val_loss: 0.7749 - val_acc: 0.8333\n",
      "Epoch 56/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0176 - acc: 0.9955 - val_loss: 0.7211 - val_acc: 0.8458\n",
      "Epoch 57/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0328 - acc: 0.9891 - val_loss: 0.7105 - val_acc: 0.8542\n",
      "Epoch 58/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0376 - acc: 0.9850 - val_loss: 0.8684 - val_acc: 0.8333\n",
      "Epoch 59/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0361 - acc: 0.9877 - val_loss: 0.7517 - val_acc: 0.8500\n",
      "Epoch 60/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0244 - acc: 0.9936 - val_loss: 0.7449 - val_acc: 0.8500\n",
      "Train on 2200 samples, validate on 240 samples\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6934 - acc: 0.5091 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 2/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6930 - acc: 0.5223 - val_loss: 0.6924 - val_acc: 0.5000\n",
      "Epoch 3/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6924 - acc: 0.5200 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 4/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6912 - acc: 0.5409 - val_loss: 0.6893 - val_acc: 0.5750\n",
      "Epoch 5/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6854 - acc: 0.5600 - val_loss: 0.6883 - val_acc: 0.5167\n",
      "Epoch 6/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6782 - acc: 0.5659 - val_loss: 0.7055 - val_acc: 0.5000\n",
      "Epoch 7/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6679 - acc: 0.5964 - val_loss: 0.6505 - val_acc: 0.6000\n",
      "Epoch 8/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.6530 - acc: 0.6123 - val_loss: 0.6455 - val_acc: 0.6375\n",
      "Epoch 9/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6461 - acc: 0.6286 - val_loss: 0.6132 - val_acc: 0.6583\n",
      "Epoch 10/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.5977 - acc: 0.6859 - val_loss: 0.5894 - val_acc: 0.6792\n",
      "Epoch 11/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.5706 - acc: 0.7073 - val_loss: 0.5118 - val_acc: 0.7583\n",
      "Epoch 12/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.5173 - acc: 0.7541 - val_loss: 0.5151 - val_acc: 0.7542\n",
      "Epoch 13/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.4710 - acc: 0.7868 - val_loss: 0.4598 - val_acc: 0.7750\n",
      "Epoch 14/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.4394 - acc: 0.8159 - val_loss: 0.4148 - val_acc: 0.7917\n",
      "Epoch 15/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3997 - acc: 0.8318 - val_loss: 0.4244 - val_acc: 0.8083\n",
      "Epoch 16/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3691 - acc: 0.8450 - val_loss: 0.3884 - val_acc: 0.8292\n",
      "Epoch 17/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3461 - acc: 0.8555 - val_loss: 0.3719 - val_acc: 0.8417\n",
      "Epoch 18/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3171 - acc: 0.8686 - val_loss: 0.3535 - val_acc: 0.8375\n",
      "Epoch 19/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2755 - acc: 0.8905 - val_loss: 0.3517 - val_acc: 0.8417\n",
      "Epoch 20/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2497 - acc: 0.8986 - val_loss: 0.3515 - val_acc: 0.8542\n",
      "Epoch 21/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2398 - acc: 0.9050 - val_loss: 0.3555 - val_acc: 0.8417\n",
      "Epoch 22/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1953 - acc: 0.9218 - val_loss: 0.3625 - val_acc: 0.8708\n",
      "Epoch 23/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1813 - acc: 0.9318 - val_loss: 0.2960 - val_acc: 0.8667\n",
      "Epoch 24/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1768 - acc: 0.9368 - val_loss: 0.2710 - val_acc: 0.8875\n",
      "Epoch 25/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1521 - acc: 0.9464 - val_loss: 0.3160 - val_acc: 0.8917\n",
      "Epoch 26/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1249 - acc: 0.9573 - val_loss: 0.3523 - val_acc: 0.8708\n",
      "Epoch 27/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1166 - acc: 0.9600 - val_loss: 0.3617 - val_acc: 0.8625\n",
      "Epoch 28/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1134 - acc: 0.9641 - val_loss: 0.3494 - val_acc: 0.8750\n",
      "Epoch 29/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1145 - acc: 0.9609 - val_loss: 0.3614 - val_acc: 0.8708\n",
      "Epoch 30/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1037 - acc: 0.9641 - val_loss: 0.3492 - val_acc: 0.8750\n",
      "Epoch 31/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0877 - acc: 0.9650 - val_loss: 0.3359 - val_acc: 0.8833\n",
      "Epoch 32/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0895 - acc: 0.9686 - val_loss: 0.3904 - val_acc: 0.8500\n",
      "Epoch 33/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0635 - acc: 0.9786 - val_loss: 0.3222 - val_acc: 0.9042\n",
      "Epoch 34/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0757 - acc: 0.9750 - val_loss: 0.4245 - val_acc: 0.8542\n",
      "Epoch 35/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0699 - acc: 0.9791 - val_loss: 0.3542 - val_acc: 0.8875\n",
      "Epoch 36/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0740 - acc: 0.9759 - val_loss: 0.3605 - val_acc: 0.8792\n",
      "Epoch 37/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0612 - acc: 0.9800 - val_loss: 0.4436 - val_acc: 0.8750\n",
      "Epoch 38/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0553 - acc: 0.9814 - val_loss: 0.4126 - val_acc: 0.8542\n",
      "Epoch 39/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0490 - acc: 0.9809 - val_loss: 0.3844 - val_acc: 0.8833\n",
      "Epoch 40/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0555 - acc: 0.9800 - val_loss: 0.4182 - val_acc: 0.8667\n",
      "Epoch 41/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0750 - acc: 0.9723 - val_loss: 0.4177 - val_acc: 0.8500\n",
      "Epoch 42/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0487 - acc: 0.9814 - val_loss: 0.4570 - val_acc: 0.8750\n",
      "Epoch 43/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0370 - acc: 0.9873 - val_loss: 0.4952 - val_acc: 0.8583\n",
      "Epoch 44/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0480 - acc: 0.9823 - val_loss: 0.4452 - val_acc: 0.8792\n",
      "Epoch 45/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0420 - acc: 0.9841 - val_loss: 0.5192 - val_acc: 0.8625\n",
      "Epoch 46/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0288 - acc: 0.9914 - val_loss: 0.6848 - val_acc: 0.8583\n",
      "Epoch 47/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0509 - acc: 0.9845 - val_loss: 0.5220 - val_acc: 0.8625\n",
      "Epoch 48/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0303 - acc: 0.9905 - val_loss: 0.5239 - val_acc: 0.8708\n",
      "Epoch 49/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0398 - acc: 0.9882 - val_loss: 0.5872 - val_acc: 0.8792\n",
      "Epoch 50/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0292 - acc: 0.9905 - val_loss: 0.5128 - val_acc: 0.8875\n",
      "Epoch 51/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0078 - acc: 0.9986 - val_loss: 0.6511 - val_acc: 0.8667\n",
      "Epoch 52/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0142 - acc: 0.9955 - val_loss: 0.6389 - val_acc: 0.8875\n",
      "Epoch 53/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0289 - acc: 0.9900 - val_loss: 0.5626 - val_acc: 0.8792\n",
      "Epoch 54/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0404 - acc: 0.9864 - val_loss: 0.5105 - val_acc: 0.8667\n",
      "Epoch 55/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0530 - acc: 0.9832 - val_loss: 0.3276 - val_acc: 0.8958\n",
      "Epoch 56/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0331 - acc: 0.9886 - val_loss: 0.4990 - val_acc: 0.8792\n",
      "Epoch 57/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0193 - acc: 0.9945 - val_loss: 0.4427 - val_acc: 0.8917\n",
      "Epoch 58/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0265 - acc: 0.9918 - val_loss: 0.6565 - val_acc: 0.8708\n",
      "Epoch 59/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0441 - acc: 0.9850 - val_loss: 0.4199 - val_acc: 0.8875\n",
      "Epoch 60/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0317 - acc: 0.9900 - val_loss: 0.3644 - val_acc: 0.8917\n",
      "Train on 2200 samples, validate on 240 samples\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6934 - acc: 0.5018 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 2/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6934 - acc: 0.5105 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 3/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6934 - acc: 0.4914 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 4/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6934 - acc: 0.4959 - val_loss: 0.6925 - val_acc: 0.5000\n",
      "Epoch 5/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6933 - acc: 0.5036 - val_loss: 0.6922 - val_acc: 0.5000\n",
      "Epoch 6/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6922 - acc: 0.5150 - val_loss: 0.6901 - val_acc: 0.5375\n",
      "Epoch 7/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6898 - acc: 0.5391 - val_loss: 0.6862 - val_acc: 0.5000\n",
      "Epoch 8/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6803 - acc: 0.5777 - val_loss: 0.6695 - val_acc: 0.6167\n",
      "Epoch 9/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6620 - acc: 0.6036 - val_loss: 0.6848 - val_acc: 0.5500\n",
      "Epoch 10/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6380 - acc: 0.6495 - val_loss: 0.6614 - val_acc: 0.6000\n",
      "Epoch 11/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6123 - acc: 0.6614 - val_loss: 0.5653 - val_acc: 0.6917\n",
      "Epoch 12/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.5348 - acc: 0.7291 - val_loss: 0.5302 - val_acc: 0.7458\n",
      "Epoch 13/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.4796 - acc: 0.7750 - val_loss: 0.4514 - val_acc: 0.7917\n",
      "Epoch 14/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.4173 - acc: 0.8168 - val_loss: 0.4271 - val_acc: 0.8083\n",
      "Epoch 15/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3638 - acc: 0.8536 - val_loss: 0.4203 - val_acc: 0.8458\n",
      "Epoch 16/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3375 - acc: 0.8600 - val_loss: 0.3854 - val_acc: 0.8292\n",
      "Epoch 17/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2948 - acc: 0.8823 - val_loss: 0.3511 - val_acc: 0.8625\n",
      "Epoch 18/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2728 - acc: 0.8955 - val_loss: 0.3334 - val_acc: 0.8667\n",
      "Epoch 19/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2296 - acc: 0.9100 - val_loss: 0.3513 - val_acc: 0.8625\n",
      "Epoch 20/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2193 - acc: 0.9164 - val_loss: 0.3236 - val_acc: 0.8833\n",
      "Epoch 21/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1962 - acc: 0.9264 - val_loss: 0.3498 - val_acc: 0.8708\n",
      "Epoch 22/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1652 - acc: 0.9395 - val_loss: 0.3063 - val_acc: 0.8792\n",
      "Epoch 23/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1483 - acc: 0.9450 - val_loss: 0.3898 - val_acc: 0.8708\n",
      "Epoch 24/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1508 - acc: 0.9400 - val_loss: 0.3506 - val_acc: 0.8833\n",
      "Epoch 25/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1333 - acc: 0.9500 - val_loss: 0.3925 - val_acc: 0.8458\n",
      "Epoch 26/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1061 - acc: 0.9559 - val_loss: 0.4084 - val_acc: 0.8875\n",
      "Epoch 27/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0902 - acc: 0.9705 - val_loss: 0.3999 - val_acc: 0.9042\n",
      "Epoch 28/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0846 - acc: 0.9691 - val_loss: 0.4158 - val_acc: 0.8833\n",
      "Epoch 29/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0698 - acc: 0.9755 - val_loss: 0.4716 - val_acc: 0.8625\n",
      "Epoch 30/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0883 - acc: 0.9714 - val_loss: 0.4844 - val_acc: 0.8833\n",
      "Epoch 31/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0585 - acc: 0.9768 - val_loss: 0.4831 - val_acc: 0.8667\n",
      "Epoch 32/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0870 - acc: 0.9709 - val_loss: 0.3872 - val_acc: 0.8833\n",
      "Epoch 33/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0595 - acc: 0.9782 - val_loss: 0.5054 - val_acc: 0.8917\n",
      "Epoch 34/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0822 - acc: 0.9705 - val_loss: 0.3829 - val_acc: 0.8667\n",
      "Epoch 35/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0527 - acc: 0.9818 - val_loss: 0.6595 - val_acc: 0.8667\n",
      "Epoch 36/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0501 - acc: 0.9818 - val_loss: 0.5313 - val_acc: 0.8833\n",
      "Epoch 37/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0530 - acc: 0.9814 - val_loss: 0.5426 - val_acc: 0.8708\n",
      "Epoch 38/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0400 - acc: 0.9855 - val_loss: 0.4761 - val_acc: 0.9042\n",
      "Epoch 39/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0392 - acc: 0.9873 - val_loss: 0.5165 - val_acc: 0.9000\n",
      "Epoch 40/60\n",
      "2200/2200 [==============================] - 5s 2ms/step - loss: 0.0258 - acc: 0.9909 - val_loss: 0.4650 - val_acc: 0.9083\n",
      "Epoch 41/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0349 - acc: 0.9886 - val_loss: 0.6070 - val_acc: 0.8958\n",
      "Epoch 42/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0677 - acc: 0.9736 - val_loss: 0.5118 - val_acc: 0.8875\n",
      "Epoch 43/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0320 - acc: 0.9891 - val_loss: 0.5810 - val_acc: 0.8750\n",
      "Epoch 44/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0587 - acc: 0.9809 - val_loss: 0.5897 - val_acc: 0.8917\n",
      "Epoch 45/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0354 - acc: 0.9859 - val_loss: 0.5210 - val_acc: 0.8917\n",
      "Epoch 46/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0210 - acc: 0.9950 - val_loss: 0.4558 - val_acc: 0.9042\n",
      "Epoch 47/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0244 - acc: 0.9905 - val_loss: 0.5983 - val_acc: 0.9000\n",
      "Epoch 48/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0288 - acc: 0.9923 - val_loss: 0.6591 - val_acc: 0.8875\n",
      "Epoch 49/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0369 - acc: 0.9855 - val_loss: 0.5181 - val_acc: 0.9042\n",
      "Epoch 50/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0222 - acc: 0.9918 - val_loss: 0.7585 - val_acc: 0.8792\n",
      "Epoch 51/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0273 - acc: 0.9882 - val_loss: 0.5784 - val_acc: 0.8750\n",
      "Epoch 52/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0531 - acc: 0.9850 - val_loss: 0.4944 - val_acc: 0.9000\n",
      "Epoch 53/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.6192 - val_acc: 0.8917\n",
      "Epoch 54/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0160 - acc: 0.9964 - val_loss: 0.7327 - val_acc: 0.8583\n",
      "Epoch 55/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0413 - acc: 0.9859 - val_loss: 0.7065 - val_acc: 0.8792\n",
      "Epoch 56/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0233 - acc: 0.9918 - val_loss: 0.5171 - val_acc: 0.8833\n",
      "Epoch 57/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0172 - acc: 0.9936 - val_loss: 0.6749 - val_acc: 0.8750\n",
      "Epoch 58/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0306 - acc: 0.9936 - val_loss: 0.5460 - val_acc: 0.8875\n",
      "Epoch 59/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0503 - acc: 0.9859 - val_loss: 0.6350 - val_acc: 0.8792\n",
      "Epoch 60/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0378 - acc: 0.9873 - val_loss: 0.7158 - val_acc: 0.8833\n",
      "Train on 2200 samples, validate on 240 samples\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6935 - acc: 0.4936 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 2/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6925 - acc: 0.5064 - val_loss: 0.6920 - val_acc: 0.5667\n",
      "Epoch 3/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6896 - acc: 0.5309 - val_loss: 0.6884 - val_acc: 0.5625\n",
      "Epoch 4/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6862 - acc: 0.5500 - val_loss: 0.6910 - val_acc: 0.5250\n",
      "Epoch 5/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6623 - acc: 0.6086 - val_loss: 0.7071 - val_acc: 0.5292\n",
      "Epoch 6/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6450 - acc: 0.6145 - val_loss: 0.5890 - val_acc: 0.7125\n",
      "Epoch 7/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.5698 - acc: 0.7014 - val_loss: 0.5119 - val_acc: 0.7375\n",
      "Epoch 8/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.4964 - acc: 0.7736 - val_loss: 0.4735 - val_acc: 0.8000\n",
      "Epoch 9/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.4447 - acc: 0.8082 - val_loss: 0.4585 - val_acc: 0.7958\n",
      "Epoch 10/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.4001 - acc: 0.8368 - val_loss: 0.4135 - val_acc: 0.8083\n",
      "Epoch 11/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3461 - acc: 0.8600 - val_loss: 0.4203 - val_acc: 0.8292\n",
      "Epoch 12/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3296 - acc: 0.8718 - val_loss: 0.3725 - val_acc: 0.8667\n",
      "Epoch 13/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3068 - acc: 0.8800 - val_loss: 0.3998 - val_acc: 0.8500\n",
      "Epoch 14/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2841 - acc: 0.8945 - val_loss: 0.3690 - val_acc: 0.8750\n",
      "Epoch 15/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2562 - acc: 0.9045 - val_loss: 0.3794 - val_acc: 0.8500\n",
      "Epoch 16/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2383 - acc: 0.9105 - val_loss: 0.3515 - val_acc: 0.8792\n",
      "Epoch 17/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2266 - acc: 0.9136 - val_loss: 0.3365 - val_acc: 0.8792\n",
      "Epoch 18/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2071 - acc: 0.9205 - val_loss: 0.3645 - val_acc: 0.8625\n",
      "Epoch 19/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1921 - acc: 0.9318 - val_loss: 0.4383 - val_acc: 0.8625\n",
      "Epoch 20/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1706 - acc: 0.9345 - val_loss: 0.3624 - val_acc: 0.8667\n",
      "Epoch 21/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1519 - acc: 0.9455 - val_loss: 0.3657 - val_acc: 0.8875\n",
      "Epoch 22/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1558 - acc: 0.9400 - val_loss: 0.3311 - val_acc: 0.8833\n",
      "Epoch 23/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1542 - acc: 0.9377 - val_loss: 0.3183 - val_acc: 0.8917\n",
      "Epoch 24/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1245 - acc: 0.9541 - val_loss: 0.3530 - val_acc: 0.9042\n",
      "Epoch 25/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1243 - acc: 0.9550 - val_loss: 0.3422 - val_acc: 0.8833\n",
      "Epoch 26/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1036 - acc: 0.9641 - val_loss: 0.3886 - val_acc: 0.8792\n",
      "Epoch 27/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1047 - acc: 0.9650 - val_loss: 0.4093 - val_acc: 0.8958\n",
      "Epoch 28/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0988 - acc: 0.9673 - val_loss: 0.4395 - val_acc: 0.8625\n",
      "Epoch 29/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0809 - acc: 0.9682 - val_loss: 0.4085 - val_acc: 0.8875\n",
      "Epoch 30/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1001 - acc: 0.9641 - val_loss: 0.4840 - val_acc: 0.8625\n",
      "Epoch 31/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0827 - acc: 0.9750 - val_loss: 0.4009 - val_acc: 0.8875\n",
      "Epoch 32/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0898 - acc: 0.9691 - val_loss: 0.3987 - val_acc: 0.8833\n",
      "Epoch 33/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0567 - acc: 0.9827 - val_loss: 0.4723 - val_acc: 0.9042\n",
      "Epoch 34/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0694 - acc: 0.9755 - val_loss: 0.4165 - val_acc: 0.9042\n",
      "Epoch 35/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0578 - acc: 0.9800 - val_loss: 0.5010 - val_acc: 0.9000\n",
      "Epoch 36/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0572 - acc: 0.9809 - val_loss: 0.5041 - val_acc: 0.8750\n",
      "Epoch 37/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0593 - acc: 0.9800 - val_loss: 0.4767 - val_acc: 0.8667\n",
      "Epoch 38/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0544 - acc: 0.9805 - val_loss: 0.3730 - val_acc: 0.8833\n",
      "Epoch 39/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0423 - acc: 0.9868 - val_loss: 0.4689 - val_acc: 0.8708\n",
      "Epoch 40/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0628 - acc: 0.9800 - val_loss: 0.4215 - val_acc: 0.9083\n",
      "Epoch 41/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0485 - acc: 0.9832 - val_loss: 0.4317 - val_acc: 0.9042\n",
      "Epoch 42/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0396 - acc: 0.9855 - val_loss: 0.4947 - val_acc: 0.8958\n",
      "Epoch 43/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0413 - acc: 0.9864 - val_loss: 0.5316 - val_acc: 0.8792\n",
      "Epoch 44/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0490 - acc: 0.9832 - val_loss: 0.5753 - val_acc: 0.8375\n",
      "Epoch 45/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0458 - acc: 0.9859 - val_loss: 0.4337 - val_acc: 0.9125\n",
      "Epoch 46/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0250 - acc: 0.9909 - val_loss: 0.6070 - val_acc: 0.8958\n",
      "Epoch 47/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0441 - acc: 0.9832 - val_loss: 0.6192 - val_acc: 0.8875\n",
      "Epoch 48/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0419 - acc: 0.9882 - val_loss: 0.6467 - val_acc: 0.8375\n",
      "Epoch 49/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0353 - acc: 0.9900 - val_loss: 0.6587 - val_acc: 0.8583\n",
      "Epoch 50/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0521 - acc: 0.9805 - val_loss: 0.7363 - val_acc: 0.8500\n",
      "Epoch 51/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0443 - acc: 0.9845 - val_loss: 0.6337 - val_acc: 0.8708\n",
      "Epoch 52/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0339 - acc: 0.9855 - val_loss: 0.7116 - val_acc: 0.8708\n",
      "Epoch 53/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0313 - acc: 0.9882 - val_loss: 0.6707 - val_acc: 0.8958\n",
      "Epoch 54/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0435 - acc: 0.9859 - val_loss: 0.6947 - val_acc: 0.8542\n",
      "Epoch 55/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0398 - acc: 0.9841 - val_loss: 0.5735 - val_acc: 0.8833\n",
      "Epoch 56/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0217 - acc: 0.9932 - val_loss: 0.6062 - val_acc: 0.9000\n",
      "Epoch 57/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0115 - acc: 0.9959 - val_loss: 0.7282 - val_acc: 0.8917\n",
      "Epoch 58/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.7285 - val_acc: 0.8958\n",
      "Epoch 59/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0591 - acc: 0.9805 - val_loss: 0.7064 - val_acc: 0.8708\n",
      "Epoch 60/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0160 - acc: 0.9964 - val_loss: 0.6111 - val_acc: 0.8958\n",
      "Train on 2200 samples, validate on 240 samples\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6941 - acc: 0.5086 - val_loss: 0.6916 - val_acc: 0.6542\n",
      "Epoch 2/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6875 - acc: 0.5486 - val_loss: 0.6776 - val_acc: 0.5208\n",
      "Epoch 3/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6645 - acc: 0.5891 - val_loss: 0.6158 - val_acc: 0.6500\n",
      "Epoch 4/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.5865 - acc: 0.6923 - val_loss: 0.5380 - val_acc: 0.7417\n",
      "Epoch 5/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.5193 - acc: 0.7455 - val_loss: 0.4940 - val_acc: 0.7542\n",
      "Epoch 6/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.4677 - acc: 0.7836 - val_loss: 0.4516 - val_acc: 0.7958\n",
      "Epoch 7/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.4221 - acc: 0.8155 - val_loss: 0.4073 - val_acc: 0.8292\n",
      "Epoch 8/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3804 - acc: 0.8386 - val_loss: 0.3575 - val_acc: 0.8708\n",
      "Epoch 9/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3467 - acc: 0.8559 - val_loss: 0.3235 - val_acc: 0.8750\n",
      "Epoch 10/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3058 - acc: 0.8750 - val_loss: 0.3050 - val_acc: 0.8667\n",
      "Epoch 11/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2817 - acc: 0.8900 - val_loss: 0.2821 - val_acc: 0.8667\n",
      "Epoch 12/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2713 - acc: 0.8850 - val_loss: 0.2608 - val_acc: 0.8958\n",
      "Epoch 13/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2401 - acc: 0.9077 - val_loss: 0.2616 - val_acc: 0.9000\n",
      "Epoch 14/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2332 - acc: 0.9055 - val_loss: 0.2661 - val_acc: 0.8958\n",
      "Epoch 15/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2012 - acc: 0.9241 - val_loss: 0.2525 - val_acc: 0.9125\n",
      "Epoch 16/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1817 - acc: 0.9309 - val_loss: 0.2617 - val_acc: 0.9125\n",
      "Epoch 17/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1763 - acc: 0.9345 - val_loss: 0.3126 - val_acc: 0.8583\n",
      "Epoch 18/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1665 - acc: 0.9368 - val_loss: 0.2581 - val_acc: 0.9125\n",
      "Epoch 19/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1463 - acc: 0.9464 - val_loss: 0.2814 - val_acc: 0.9083\n",
      "Epoch 20/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1371 - acc: 0.9495 - val_loss: 0.2716 - val_acc: 0.9083\n",
      "Epoch 21/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1203 - acc: 0.9586 - val_loss: 0.2937 - val_acc: 0.9000\n",
      "Epoch 22/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1135 - acc: 0.9591 - val_loss: 0.2767 - val_acc: 0.9083\n",
      "Epoch 23/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1158 - acc: 0.9555 - val_loss: 0.2890 - val_acc: 0.9083\n",
      "Epoch 24/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0825 - acc: 0.9741 - val_loss: 0.3061 - val_acc: 0.8958\n",
      "Epoch 25/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1110 - acc: 0.9614 - val_loss: 0.2760 - val_acc: 0.9042\n",
      "Epoch 26/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0906 - acc: 0.9682 - val_loss: 0.3085 - val_acc: 0.9042\n",
      "Epoch 27/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0833 - acc: 0.9673 - val_loss: 0.3432 - val_acc: 0.9042\n",
      "Epoch 28/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0754 - acc: 0.9777 - val_loss: 0.2946 - val_acc: 0.9167\n",
      "Epoch 29/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0545 - acc: 0.9832 - val_loss: 0.3887 - val_acc: 0.9000\n",
      "Epoch 30/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0620 - acc: 0.9782 - val_loss: 0.3613 - val_acc: 0.9000\n",
      "Epoch 31/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0564 - acc: 0.9791 - val_loss: 0.3938 - val_acc: 0.9125\n",
      "Epoch 32/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0611 - acc: 0.9786 - val_loss: 0.3538 - val_acc: 0.8917\n",
      "Epoch 33/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0354 - acc: 0.9855 - val_loss: 0.3979 - val_acc: 0.9042\n",
      "Epoch 34/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0492 - acc: 0.9832 - val_loss: 0.3109 - val_acc: 0.9083\n",
      "Epoch 35/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0429 - acc: 0.9855 - val_loss: 0.4408 - val_acc: 0.9042\n",
      "Epoch 36/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0606 - acc: 0.9805 - val_loss: 0.3916 - val_acc: 0.8875\n",
      "Epoch 37/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0477 - acc: 0.9809 - val_loss: 0.3168 - val_acc: 0.9125\n",
      "Epoch 38/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0482 - acc: 0.9814 - val_loss: 0.4462 - val_acc: 0.9083\n",
      "Epoch 39/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0389 - acc: 0.9836 - val_loss: 0.4105 - val_acc: 0.9333\n",
      "Epoch 40/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0510 - acc: 0.9832 - val_loss: 0.3475 - val_acc: 0.9125\n",
      "Epoch 41/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0329 - acc: 0.9873 - val_loss: 0.4071 - val_acc: 0.9042\n",
      "Epoch 42/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0345 - acc: 0.9891 - val_loss: 0.4050 - val_acc: 0.9083\n",
      "Epoch 43/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0165 - acc: 0.9955 - val_loss: 0.3861 - val_acc: 0.9000\n",
      "Epoch 44/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0488 - acc: 0.9855 - val_loss: 0.3615 - val_acc: 0.9000\n",
      "Epoch 45/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0433 - acc: 0.9845 - val_loss: 0.4811 - val_acc: 0.9208\n",
      "Epoch 46/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0428 - acc: 0.9836 - val_loss: 0.5139 - val_acc: 0.8917\n",
      "Epoch 47/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0353 - acc: 0.9886 - val_loss: 0.3612 - val_acc: 0.9167\n",
      "Epoch 48/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0267 - acc: 0.9909 - val_loss: 0.3681 - val_acc: 0.9167\n",
      "Epoch 49/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0300 - acc: 0.9900 - val_loss: 0.4313 - val_acc: 0.9250\n",
      "Epoch 50/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0173 - acc: 0.9932 - val_loss: 0.5158 - val_acc: 0.9167\n",
      "Epoch 51/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0312 - acc: 0.9923 - val_loss: 0.4031 - val_acc: 0.8958\n",
      "Epoch 52/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0292 - acc: 0.9877 - val_loss: 0.3658 - val_acc: 0.9250\n",
      "Epoch 53/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0214 - acc: 0.9927 - val_loss: 0.3860 - val_acc: 0.9333\n",
      "Epoch 54/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0427 - acc: 0.9873 - val_loss: 0.3024 - val_acc: 0.9417\n",
      "Epoch 55/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0239 - acc: 0.9914 - val_loss: 0.4926 - val_acc: 0.9042\n",
      "Epoch 56/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0227 - acc: 0.9927 - val_loss: 0.5004 - val_acc: 0.8958\n",
      "Epoch 57/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0251 - acc: 0.9914 - val_loss: 0.3634 - val_acc: 0.9208\n",
      "Epoch 58/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0221 - acc: 0.9918 - val_loss: 0.4094 - val_acc: 0.9167\n",
      "Epoch 59/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0354 - acc: 0.9859 - val_loss: 0.4126 - val_acc: 0.9208\n",
      "Epoch 60/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0137 - acc: 0.9950 - val_loss: 0.5287 - val_acc: 0.9083\n",
      "Train on 2200 samples, validate on 240 samples\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6939 - acc: 0.4955 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 2/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6939 - acc: 0.4955 - val_loss: 0.6928 - val_acc: 0.5000\n",
      "Epoch 3/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6926 - acc: 0.5236 - val_loss: 0.6917 - val_acc: 0.5958\n",
      "Epoch 4/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6905 - acc: 0.5400 - val_loss: 0.6878 - val_acc: 0.5458\n",
      "Epoch 5/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6783 - acc: 0.5627 - val_loss: 0.7128 - val_acc: 0.5000\n",
      "Epoch 6/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6949 - acc: 0.5068 - val_loss: 0.6927 - val_acc: 0.5000\n",
      "Epoch 7/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6737 - acc: 0.5627 - val_loss: 0.6269 - val_acc: 0.6250\n",
      "Epoch 8/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.6225 - acc: 0.6491 - val_loss: 0.5916 - val_acc: 0.6708\n",
      "Epoch 9/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.5606 - acc: 0.7068 - val_loss: 0.5131 - val_acc: 0.7292\n",
      "Epoch 10/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.4815 - acc: 0.7750 - val_loss: 0.4753 - val_acc: 0.7583\n",
      "Epoch 11/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.4145 - acc: 0.8241 - val_loss: 0.4439 - val_acc: 0.7958\n",
      "Epoch 12/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3757 - acc: 0.8400 - val_loss: 0.4962 - val_acc: 0.7833\n",
      "Epoch 13/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.3306 - acc: 0.8641 - val_loss: 0.3937 - val_acc: 0.8292\n",
      "Epoch 14/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2925 - acc: 0.8882 - val_loss: 0.4028 - val_acc: 0.8208\n",
      "Epoch 15/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2627 - acc: 0.8982 - val_loss: 0.3607 - val_acc: 0.8208\n",
      "Epoch 16/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2672 - acc: 0.8941 - val_loss: 0.3591 - val_acc: 0.8542\n",
      "Epoch 17/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2231 - acc: 0.9177 - val_loss: 0.3335 - val_acc: 0.8708\n",
      "Epoch 18/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.2108 - acc: 0.9223 - val_loss: 0.4218 - val_acc: 0.8583\n",
      "Epoch 19/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1888 - acc: 0.9345 - val_loss: 0.3747 - val_acc: 0.8500\n",
      "Epoch 20/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1643 - acc: 0.9377 - val_loss: 0.3774 - val_acc: 0.8458\n",
      "Epoch 21/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1572 - acc: 0.9418 - val_loss: 0.5124 - val_acc: 0.8583\n",
      "Epoch 22/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1345 - acc: 0.9509 - val_loss: 0.4360 - val_acc: 0.8542\n",
      "Epoch 23/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1285 - acc: 0.9527 - val_loss: 0.4127 - val_acc: 0.8667\n",
      "Epoch 24/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.1117 - acc: 0.9550 - val_loss: 0.4168 - val_acc: 0.8583\n",
      "Epoch 25/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0894 - acc: 0.9695 - val_loss: 0.4510 - val_acc: 0.8625\n",
      "Epoch 26/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0925 - acc: 0.9668 - val_loss: 0.4501 - val_acc: 0.8542\n",
      "Epoch 27/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0730 - acc: 0.9750 - val_loss: 0.5638 - val_acc: 0.8542\n",
      "Epoch 28/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0712 - acc: 0.9773 - val_loss: 0.6551 - val_acc: 0.8417\n",
      "Epoch 29/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0841 - acc: 0.9682 - val_loss: 0.5117 - val_acc: 0.8458\n",
      "Epoch 30/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0564 - acc: 0.9823 - val_loss: 0.6667 - val_acc: 0.8375\n",
      "Epoch 31/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0703 - acc: 0.9768 - val_loss: 0.6674 - val_acc: 0.8583\n",
      "Epoch 32/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0740 - acc: 0.9736 - val_loss: 0.5366 - val_acc: 0.8542\n",
      "Epoch 33/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0520 - acc: 0.9823 - val_loss: 0.7416 - val_acc: 0.8375\n",
      "Epoch 34/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0433 - acc: 0.9845 - val_loss: 0.6477 - val_acc: 0.8500\n",
      "Epoch 35/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0431 - acc: 0.9859 - val_loss: 0.7220 - val_acc: 0.8500\n",
      "Epoch 36/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0501 - acc: 0.9827 - val_loss: 0.7420 - val_acc: 0.8417\n",
      "Epoch 37/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0440 - acc: 0.9823 - val_loss: 0.6653 - val_acc: 0.8458\n",
      "Epoch 38/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0321 - acc: 0.9923 - val_loss: 0.7274 - val_acc: 0.8542\n",
      "Epoch 39/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0356 - acc: 0.9868 - val_loss: 0.6896 - val_acc: 0.8583\n",
      "Epoch 40/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0276 - acc: 0.9914 - val_loss: 0.8244 - val_acc: 0.8708\n",
      "Epoch 41/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0582 - acc: 0.9805 - val_loss: 0.6299 - val_acc: 0.8417\n",
      "Epoch 42/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0403 - acc: 0.9877 - val_loss: 0.6566 - val_acc: 0.8542\n",
      "Epoch 43/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0273 - acc: 0.9900 - val_loss: 0.8482 - val_acc: 0.8458\n",
      "Epoch 44/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0327 - acc: 0.9873 - val_loss: 0.8228 - val_acc: 0.8458\n",
      "Epoch 45/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0325 - acc: 0.9864 - val_loss: 0.7877 - val_acc: 0.8500\n",
      "Epoch 46/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0428 - acc: 0.9859 - val_loss: 0.7479 - val_acc: 0.8458\n",
      "Epoch 47/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0319 - acc: 0.9895 - val_loss: 0.7341 - val_acc: 0.8542\n",
      "Epoch 48/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0148 - acc: 0.9950 - val_loss: 0.7174 - val_acc: 0.8583\n",
      "Epoch 49/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0146 - acc: 0.9945 - val_loss: 0.8491 - val_acc: 0.8500\n",
      "Epoch 50/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0340 - acc: 0.9873 - val_loss: 0.9982 - val_acc: 0.8500\n",
      "Epoch 51/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0335 - acc: 0.9886 - val_loss: 0.6644 - val_acc: 0.8583\n",
      "Epoch 52/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0274 - acc: 0.9918 - val_loss: 0.7535 - val_acc: 0.8750\n",
      "Epoch 53/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0218 - acc: 0.9923 - val_loss: 0.6927 - val_acc: 0.8750\n",
      "Epoch 54/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0236 - acc: 0.9918 - val_loss: 1.0297 - val_acc: 0.8667\n",
      "Epoch 55/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0210 - acc: 0.9945 - val_loss: 1.0692 - val_acc: 0.8375\n",
      "Epoch 56/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0206 - acc: 0.9923 - val_loss: 0.8712 - val_acc: 0.8750\n",
      "Epoch 57/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0403 - acc: 0.9859 - val_loss: 0.7197 - val_acc: 0.8583\n",
      "Epoch 58/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0371 - acc: 0.9891 - val_loss: 0.5053 - val_acc: 0.8750\n",
      "Epoch 59/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0140 - acc: 0.9968 - val_loss: 0.7725 - val_acc: 0.8750\n",
      "Epoch 60/60\n",
      "2200/2200 [==============================] - 4s 2ms/step - loss: 0.0170 - acc: 0.9927 - val_loss: 0.8364 - val_acc: 0.8667\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "#model.save_weights(\"init_weights.h5\")\n",
    "\n",
    "\n",
    "acc = []\n",
    "val_acc = []\n",
    "\n",
    "for fold in range(10):\n",
    "    model.load_weights(\"init_weights.h5\")\n",
    "    \n",
    "    history = model.fit(train_x[:, :, :, :, fold], conf_train_y[:, :, fold], epochs=60, batch_size=batch_size, validation_data=(test_x[:, :, :, :, fold], conf_test_y[:, :, fold]))\n",
    "    \n",
    "    acc.append(history.history['acc'])\n",
    "    val_acc.append(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(model.layers[1].get_weights())[0, :, :, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(model.layers[2].get_weights())[0, :, :, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = []\n",
    "\n",
    "for accs in val_acc:\n",
    "    lst = np.asarray(np.argsort(accs)[-5:])\n",
    "    acc_score.append(np.mean(np.take(accs, lst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9000833282868067"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_HOG = model_flatten.predict(train_x, batch_size=batch_size)\n",
    "#test_HOG = model_flatten.predict(test_x, batch_size=batch_size)\n",
    "#print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "#clf = SGDClassifier(max_iter=1000)\n",
    "#clf = clf.fit(train_HOG, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score = clf.score(test_HOG, test_y)\n",
    "#score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_conv_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
